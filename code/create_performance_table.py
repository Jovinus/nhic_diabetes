"""
ë…¼ë¬¸ìš© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” ìƒì„± ìŠ¤í¬ë¦½íŠ¸
- Youden Index ê¸°ë°˜ ìµœì  threshold
- Bootstrap 95% CI
- AUROC, AUPRC, Accuracy, Sensitivity, Specificity, PPV, NPV
"""

import os
import numpy as np
import pandas as pd
import pickle
import json
from typing import Dict, Any, List, Tuple, Optional
from sklearn.metrics import (
    roc_auc_score, average_precision_score, accuracy_score,
    precision_score, recall_score, f1_score,
    confusion_matrix, roc_curve
)
import warnings
warnings.filterwarnings('ignore')


def find_optimal_threshold_youden(y_true: np.ndarray, y_prob: np.ndarray) -> Tuple[float, float]:
    """
    Youden Indexë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  threshold ì°¾ê¸°
    
    Youden Index = Sensitivity + Specificity - 1 = TPR - FPR
    
    Args:
        y_true: ì‹¤ì œ ë ˆì´ë¸”
        y_prob: ì˜ˆì¸¡ í™•ë¥ 
        
    Returns:
        (optimal_threshold, youden_index)
    """
    fpr, tpr, thresholds = roc_curve(y_true, y_prob)
    
    # Youden Index ê³„ì‚°
    youden = tpr - fpr
    
    # ìµœëŒ€ Youden Indexì˜ ì¸ë±ìŠ¤
    optimal_idx = np.argmax(youden)
    optimal_threshold = thresholds[optimal_idx]
    optimal_youden = youden[optimal_idx]
    
    return optimal_threshold, optimal_youden


def calculate_metrics_at_threshold(
    y_true: np.ndarray,
    y_prob: np.ndarray,
    threshold: float
) -> Dict[str, float]:
    """
    íŠ¹ì • thresholdì—ì„œ ë¶„ë¥˜ ì§€í‘œ ê³„ì‚°
    
    Args:
        y_true: ì‹¤ì œ ë ˆì´ë¸”
        y_prob: ì˜ˆì¸¡ í™•ë¥ 
        threshold: ë¶„ë¥˜ ì„ê³„ê°’
        
    Returns:
        ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    """
    y_pred = (y_prob >= threshold).astype(int)
    
    # Confusion Matrix
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    
    metrics = {
        'auroc': roc_auc_score(y_true, y_prob),
        'auprc': average_precision_score(y_true, y_prob),
        'accuracy': (tp + tn) / (tp + tn + fp + fn),
        'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,  # Recall
        'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,
        'ppv': tp / (tp + fp) if (tp + fp) > 0 else 0,  # Precision
        'npv': tn / (tn + fn) if (tn + fn) > 0 else 0,
        'f1': 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0,
        'threshold': threshold
    }
    
    return metrics


def bootstrap_metrics(
    y_true: np.ndarray,
    y_prob: np.ndarray,
    n_bootstrap: int = 1000,
    ci_level: float = 0.95,
    use_youden: bool = True,
    random_state: int = 1004
) -> Dict[str, Dict[str, float]]:
    """
    Bootstrapì„ ì‚¬ìš©í•œ ì„±ëŠ¥ ì§€í‘œ ë° ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
    
    Args:
        y_true: ì‹¤ì œ ë ˆì´ë¸”
        y_prob: ì˜ˆì¸¡ í™•ë¥ 
        n_bootstrap: bootstrap ë°˜ë³µ íšŸìˆ˜
        ci_level: ì‹ ë¢°êµ¬ê°„ ìˆ˜ì¤€ (ê¸°ë³¸ 95%)
        use_youden: Youden index ê¸°ë°˜ threshold ì‚¬ìš© ì—¬ë¶€
        random_state: ëœë¤ ì‹œë“œ
        
    Returns:
        ê° ì§€í‘œë³„ point estimate, lower CI, upper CI
    """
    np.random.seed(random_state)
    n_samples = len(y_true)
    
    # ì›ë³¸ ë°ì´í„°ì—ì„œ ìµœì  threshold ê³„ì‚° (Youden Index)
    if use_youden:
        optimal_threshold, _ = find_optimal_threshold_youden(y_true, y_prob)
    else:
        optimal_threshold = 0.5
    
    # ì›ë³¸ ì§€í‘œ ê³„ì‚°
    original_metrics = calculate_metrics_at_threshold(y_true, y_prob, optimal_threshold)
    
    # Bootstrap ìƒ˜í”Œë§
    bootstrap_results = {metric: [] for metric in original_metrics.keys()}
    
    for i in range(n_bootstrap):
        # ë³µì› ì¶”ì¶œ
        indices = np.random.choice(n_samples, n_samples, replace=True)
        y_true_boot = y_true[indices]
        y_prob_boot = y_prob[indices]
        
        # í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        if len(np.unique(y_true_boot)) < 2:
            continue
        
        # ê° bootstrap ìƒ˜í”Œì—ì„œë„ Youden threshold ì¬ê³„ì‚° (ë” ì •í™•í•œ CI)
        if use_youden:
            boot_threshold, _ = find_optimal_threshold_youden(y_true_boot, y_prob_boot)
        else:
            boot_threshold = optimal_threshold
        
        boot_metrics = calculate_metrics_at_threshold(y_true_boot, y_prob_boot, boot_threshold)
        
        for metric, value in boot_metrics.items():
            bootstrap_results[metric].append(value)
    
    # CI ê³„ì‚°
    alpha = 1 - ci_level
    results = {}
    
    for metric in original_metrics.keys():
        values = np.array(bootstrap_results[metric])
        if len(values) > 0:
            lower = np.percentile(values, alpha / 2 * 100)
            upper = np.percentile(values, (1 - alpha / 2) * 100)
        else:
            lower = upper = original_metrics[metric]
        
        results[metric] = {
            'point': original_metrics[metric],
            'lower': lower,
            'upper': upper
        }
    
    return results


def format_metric_with_ci(point: float, lower: float, upper: float, decimals: int = 3) -> str:
    """ì§€í‘œë¥¼ CIì™€ í•¨ê»˜ í¬ë§·íŒ…"""
    return f"{point:.{decimals}f} ({lower:.{decimals}f}-{upper:.{decimals}f})"


def load_model(model_path: str) -> Any:
    """ëª¨ë¸ ë¡œë“œ (ëª¨ë“  ëª¨ë¸ pklë¡œ í†µì¼)"""
    try:
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        return model
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {os.path.basename(model_path)} - {e}")
        return None


def evaluate_single_model(
    model_path: str,
    X_test: np.ndarray,
    y_test: np.ndarray,
    n_bootstrap: int = 1000,
    model_name: str = None
) -> Dict[str, Dict[str, float]]:
    """
    ë‹¨ì¼ ëª¨ë¸ í‰ê°€ (Bootstrap CI í¬í•¨)
    
    Args:
        model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        X_test: í…ŒìŠ¤íŠ¸ íŠ¹ì„±
        y_test: í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ
        n_bootstrap: Bootstrap ë°˜ë³µ íšŸìˆ˜
        model_name: ëª¨ë¸ ì´ë¦„ (ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ)
        
    Returns:
        Bootstrap CI í¬í•¨ ì„±ëŠ¥ ì§€í‘œ
    """
    # ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
    if model_name is None:
        model_name = os.path.basename(model_path)
        model_name = model_name.replace('_best_model', '').replace('_model', '')
        model_name = model_name.replace('.pkl', '').replace('.json', '').replace('.cbm', '').replace('.txt', '')
    
    # ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡
    model = load_model(model_path)
    
    if model is None:
        print(f"   âš ï¸ {model_name}: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
        return None
    
    if hasattr(model, 'predict_proba'):
        y_prob = model.predict_proba(X_test)[:, 1]
    else:
        y_prob = model.predict(X_test)
    
    # Bootstrap í‰ê°€
    print(f"   ğŸ”„ {model_name}: Bootstrap CI ê³„ì‚° ì¤‘ (n={n_bootstrap})...")
    results = bootstrap_metrics(y_test, y_prob, n_bootstrap=n_bootstrap, use_youden=True)
    
    return results


def create_performance_table(
    model_paths: List[str],
    data_dir: str = '../data/processed',
    n_bootstrap: int = 1000,
    output_path: str = None,
    model_names: List[str] = None
) -> pd.DataFrame:
    """
    ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” ìƒì„±
    
    Args:
        model_paths: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        data_dir: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬
        n_bootstrap: Bootstrap ë°˜ë³µ íšŸìˆ˜
        output_path: ì €ì¥ ê²½ë¡œ
        model_names: ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ)
        
    Returns:
        ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” DataFrame
    """
    # ë°ì´í„° ë¡œë“œ
    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))
    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))
    
    print("=" * 70)
    print("ğŸ“Š ë…¼ë¬¸ìš© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” ìƒì„±")
    print("=" * 70)
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]} ìƒ˜í”Œ")
    print(f"Bootstrap ë°˜ë³µ: {n_bootstrap}")
    print(f"Threshold: Youden Index ê¸°ì¤€")
    print()
    
    # ê° ëª¨ë¸ í‰ê°€
    all_results = {}
    
    for i, model_path in enumerate(model_paths):
        if not os.path.exists(model_path):
            print(f"   âš ï¸ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
            continue
        
        name = model_names[i] if model_names and i < len(model_names) else None
        if name is None:
            name = os.path.basename(model_path)
            name = name.replace('_best_model', '').replace('_model', '')
            name = name.replace('.pkl', '').replace('.json', '').replace('.cbm', '').replace('.txt', '')
        
        results = evaluate_single_model(model_path, X_test, y_test, n_bootstrap, name)
        if results is not None:
            all_results[name] = results
    
    # í…Œì´ë¸” ìƒì„±
    metrics_order = ['auroc', 'auprc', 'accuracy', 'sensitivity', 'specificity', 'ppv', 'npv', 'f1', 'threshold']
    metrics_labels = {
        'auroc': 'AUROC',
        'auprc': 'AUPRC',
        'accuracy': 'Accuracy',
        'sensitivity': 'Sensitivity',
        'specificity': 'Specificity',
        'ppv': 'PPV',
        'npv': 'NPV',
        'f1': 'F1 Score',
        'threshold': 'Optimal Threshold'
    }
    
    # ëª¨ë¸ í‘œì‹œ ì´ë¦„ ë§¤í•‘
    display_names = {
        'decision_tree': 'Decision Tree',
        'random_forest': 'Random Forest',
        'xgboost': 'XGBoost',
        'lightgbm': 'LightGBM',
        'ann': 'MLP',
    }
    
    # DataFrame êµ¬ì„±
    rows = []
    for model_name, results in all_results.items():
        row = {'Model': display_names.get(model_name, model_name)}
        for metric in metrics_order:
            if metric in results:
                m = results[metric]
                if metric == 'threshold':
                    row[metrics_labels[metric]] = f"{m['point']:.3f}"
                else:
                    row[metrics_labels[metric]] = format_metric_with_ci(m['point'], m['lower'], m['upper'])
        rows.append(row)
    
    df = pd.DataFrame(rows)
    
    # ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ“‹ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” (95% Bootstrap CI)")
    print("=" * 70)
    print(df.to_string(index=False))
    
    # ì €ì¥
    if output_path:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        if output_path.endswith('.xlsx'):
            # Excel ì €ì¥ ì‹œ í¬ë§·íŒ…
            df.to_excel(output_path, index=False)
        elif output_path.endswith('.csv'):
            df.to_csv(output_path, index=False)
        elif output_path.endswith('.tex'):
            # LaTeX í…Œì´ë¸”
            latex_str = df.to_latex(index=False, escape=False)
            with open(output_path, 'w') as f:
                f.write(latex_str)
        else:
            df.to_csv(output_path + '.csv', index=False)
        
        print(f"\nâœ… í…Œì´ë¸” ì €ì¥: {output_path}")
    
    return df


def create_detailed_results(
    model_paths: List[str],
    data_dir: str = '../data/processed',
    n_bootstrap: int = 1000,
    output_dir: str = '../results/tables'
) -> Dict[str, pd.DataFrame]:
    """
    ìƒì„¸ ê²°ê³¼ í…Œì´ë¸” ìƒì„± (Excel ì—¬ëŸ¬ ì‹œíŠ¸)
    
    Args:
        model_paths: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        data_dir: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬
        n_bootstrap: Bootstrap ë°˜ë³µ íšŸìˆ˜
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        
    Returns:
        ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ë©”ì¸ í…Œì´ë¸”
    main_table = create_performance_table(
        model_paths, data_dir, n_bootstrap,
        output_path=os.path.join(output_dir, 'model_performance_comparison.xlsx')
    )
    
    # ì¶”ê°€: Point estimatesë§Œ ìˆëŠ” ê°„ë‹¨í•œ í…Œì´ë¸”
    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))
    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))
    
    simple_rows = []
    for model_path in model_paths:
        if not os.path.exists(model_path):
            continue
        
        model = load_model(model_path)
        model_name = os.path.basename(model_path).replace('_best_model', '').replace('_model', '')
        model_name = model_name.replace('.pkl', '').replace('.json', '').replace('.cbm', '').replace('.txt', '')
        
        if hasattr(model, 'predict_proba'):
            y_prob = model.predict_proba(X_test)[:, 1]
        else:
            y_prob = model.predict(X_test)
        
        threshold, youden = find_optimal_threshold_youden(y_test, y_prob)
        metrics = calculate_metrics_at_threshold(y_test, y_prob, threshold)
        metrics['model'] = model_name
        metrics['youden_index'] = youden
        simple_rows.append(metrics)
    
    simple_df = pd.DataFrame(simple_rows)
    cols = ['model', 'auroc', 'auprc', 'accuracy', 'sensitivity', 'specificity', 
            'ppv', 'npv', 'f1', 'threshold', 'youden_index']
    simple_df = simple_df[[c for c in cols if c in simple_df.columns]]
    
    simple_path = os.path.join(output_dir, 'model_performance_simple.csv')
    simple_df.to_csv(simple_path, index=False)
    print(f"\nâœ… ê°„ë‹¨í•œ í…Œì´ë¸” ì €ì¥: {simple_path}")
    
    return {'main': main_table, 'simple': simple_df}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    import glob
    
    parser = argparse.ArgumentParser(description='ë…¼ë¬¸ìš© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” ìƒì„±')
    parser.add_argument('--models-dir', type=str, default='../models',
                        help='ëª¨ë¸ íŒŒì¼ ë””ë ‰í† ë¦¬')
    parser.add_argument('--data-dir', type=str, default='../data/processed',
                        help='ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', type=str, default='../results/tables/model_performance.xlsx',
                        help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--n-bootstrap', type=int, default=1000,
                        help='Bootstrap ë°˜ë³µ íšŸìˆ˜')
    parser.add_argument('--models', type=str, nargs='+', default=None,
                        help='í‰ê°€í•  ëª¨ë¸ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ models-dirì—ì„œ ìë™ ê²€ìƒ‰)')
    
    args = parser.parse_args()
    
    # ëª¨ë¸ íŒŒì¼ ì°¾ê¸°
    if args.models:
        model_paths = args.models
    else:
        # ìë™ ê²€ìƒ‰
        patterns = ['*.json', '*.pkl', '*.cbm']
        model_paths = []
        for pattern in patterns:
            model_paths.extend(glob.glob(os.path.join(args.models_dir, f'*best_model{pattern}')))
            model_paths.extend(glob.glob(os.path.join(args.models_dir, f'*_model{pattern}')))
        
        # ì¤‘ë³µ ì œê±° ë° meta íŒŒì¼ ì œì™¸
        model_paths = list(set(model_paths))
        model_paths = [p for p in model_paths if 'meta' not in p]
    
    if not model_paths:
        print("âŒ í‰ê°€í•  ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ê²€ìƒ‰ ê²½ë¡œ: {args.models_dir}")
        return
    
    print(f"ğŸ“‚ ë°œê²¬ëœ ëª¨ë¸ íŒŒì¼: {len(model_paths)}ê°œ")
    for p in model_paths:
        print(f"   - {os.path.basename(p)}")
    
    # í…Œì´ë¸” ìƒì„±
    create_performance_table(
        model_paths=model_paths,
        data_dir=args.data_dir,
        n_bootstrap=args.n_bootstrap,
        output_path=args.output
    )


if __name__ == '__main__':
    main()
