"""
ëª¨ë¸ í‰ê°€ ë° í•´ì„ ìŠ¤í¬ë¦½íŠ¸
- AUROC, AUPRC, ì •í™•ë„, ë¯¼ê°ë„, íŠ¹ì´ë„ ë“±
- SHAP ë¶„ì„
- Calibration ë¶„ì„
- Feature Importance
"""

import os
import numpy as np
import pandas as pd
import pickle
import json
from typing import Dict, Any, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# Missing Indicator ì ‘ë¯¸ì‚¬ (preprocessing.pyì™€ ë™ì¼)
MISSING_INDICATOR_SUFFIX = '_missing'


def convert_to_serializable(obj):
    """numpy/pandas íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
    if isinstance(obj, dict):
        return {k: convert_to_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(v) for v in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif pd.isna(obj):
        return None
    else:
        return obj

# í‰ê°€ ì§€í‘œ
from sklearn.metrics import (
    roc_auc_score, average_precision_score, accuracy_score,
    precision_score, recall_score, f1_score,
    confusion_matrix, classification_report,
    roc_curve, precision_recall_curve,
    brier_score_loss
)
from sklearn.calibration import calibration_curve

# ì‹œê°í™”
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS, Windows, Linux ëŒ€ì‘)
def set_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    import platform
    import subprocess
    system = platform.system()
    
    font_set = False
    
    if system == 'Darwin':  # macOS
        font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'
        if os.path.exists(font_path):
            fm.fontManager.addfont(font_path)
            plt.rcParams['font.family'] = 'AppleGothic'
            font_set = True
    elif system == 'Windows':
        plt.rcParams['font.family'] = 'Malgun Gothic'
        font_set = True
    else:  # Linux
        # NanumGothic í°íŠ¸ ê²½ë¡œ í™•ì¸
        nanum_paths = [
            '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',
            '/usr/share/fonts/nanum/NanumGothic.ttf',
            os.path.expanduser('~/.fonts/NanumGothic.ttf')
        ]
        for font_path in nanum_paths:
            if os.path.exists(font_path):
                fm.fontManager.addfont(font_path)
                plt.rcParams['font.family'] = 'NanumGothic'
                font_set = True
                break
        
        # DejaVu Sansë¡œ fallback (í•œê¸€ ë¯¸ì§€ì›, í•˜ì§€ë§Œ ì—ëŸ¬ ë°©ì§€)
        if not font_set:
            available_fonts = [f.name for f in fm.fontManager.ttflist]
            fallback_fonts = ['DejaVu Sans', 'Liberation Sans', 'FreeSans', 'sans-serif']
            for font in fallback_fonts:
                if font in available_fonts or font == 'sans-serif':
                    plt.rcParams['font.family'] = font
                    font_set = True
                    break
    
    plt.rcParams['axes.unicode_minus'] = False

set_korean_font()

# numpy í˜¸í™˜ì„± íŒ¨ì¹˜ (shap 0.32 + numpy>=1.24)
# shap 0.32ê°€ ë‚´ë¶€ì ìœ¼ë¡œ np.int, np.float, np.bool ì‚¬ìš©
if not hasattr(np, 'int'):
    np.int = int
if not hasattr(np, 'float'):
    np.float = float
if not hasattr(np, 'bool'):
    np.bool = bool
if not hasattr(np, 'str'):
    np.str = str
if not hasattr(np, 'object'):
    np.object = object

# SHAP
try:
    import shap
    HAS_SHAP = True
except ImportError:
    HAS_SHAP = False
    print("âš ï¸ SHAP ë¯¸ì„¤ì¹˜ - SHAP ë¶„ì„ ë¶ˆê°€")


def find_optimal_threshold_youden(y_true: np.ndarray, y_prob: np.ndarray) -> Tuple[float, float]:
    """
    Youden Indexë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  threshold ì°¾ê¸°
    
    Youden Index = Sensitivity + Specificity - 1 = TPR - FPR
    
    Args:
        y_true: ì‹¤ì œ ë ˆì´ë¸”
        y_prob: ì˜ˆì¸¡ í™•ë¥ 
        
    Returns:
        (optimal_threshold, youden_index)
    """
    fpr, tpr, thresholds = roc_curve(y_true, y_prob)
    
    # Youden Index ê³„ì‚°
    youden = tpr - fpr
    
    # ìµœëŒ€ Youden Indexì˜ ì¸ë±ìŠ¤
    optimal_idx = np.argmax(youden)
    optimal_threshold = thresholds[optimal_idx]
    optimal_youden = youden[optimal_idx]
    
    return optimal_threshold, optimal_youden


def _save_figure(fig, save_path, dpi=500, bbox_inches='tight', pad_inches=0.1):
    """Figureë¥¼ png, tiff, pdf 3ì¢…ìœ¼ë¡œ ì €ì¥"""
    import os
    base, _ = os.path.splitext(save_path)
    for fmt in ['png', 'tiff', 'pdf']:
        out = f"{base}.{fmt}"
        fig.savefig(out, dpi=dpi, bbox_inches=bbox_inches, pad_inches=pad_inches, format=fmt)
    print(f"âœ… Figure ì €ì¥: {base}.{{png,tiff,pdf}}")


class ModelEvaluator:
    """ëª¨ë¸ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        model: Any,
        feature_names: List[str] = None,
        model_name: str = 'model'
    ):
        """
        Args:
            model: í•™ìŠµëœ ëª¨ë¸
            feature_names: íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            model_name: ëª¨ë¸ ì´ë¦„
        """
        # ëª¨ë¸ í‘œì‹œ ì´ë¦„ ë§¤í•‘
        display_names = {
            'ann': 'MLP',
            'decision_tree': 'Decision Tree',
            'random_forest': 'Random Forest',
            'xgboost': 'XGBoost',
            'lightgbm': 'LightGBM',
        }
        self.model = model
        self.feature_names = feature_names
        self.model_name = display_names.get(model_name, model_name)
        self.results = {}
        self.optimal_threshold = None
        self.youden_index = None
        
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """ì˜ˆì¸¡ í™•ë¥  ë°˜í™˜"""
        if hasattr(self.model, 'predict_proba'):
            return self.model.predict_proba(X)[:, 1]
        else:
            return self.model.predict(X)
    
    def find_optimal_threshold(self, X: np.ndarray, y: np.ndarray) -> float:
        """
        Youden Index ê¸°ë°˜ ìµœì  threshold ì°¾ê¸°
        
        Args:
            X: íŠ¹ì„± ë°°ì—´
            y: íƒ€ê²Ÿ ë°°ì—´
            
        Returns:
            ìµœì  threshold
        """
        y_prob = self.predict_proba(X)
        self.optimal_threshold, self.youden_index = find_optimal_threshold_youden(y, y_prob)
        return self.optimal_threshold
    
    def evaluate(
        self,
        X: np.ndarray,
        y: np.ndarray,
        threshold: float = None,
        use_youden: bool = True
    ) -> Dict[str, float]:
        """
        ëª¨ë¸ í‰ê°€
        
        Args:
            X: íŠ¹ì„± ë°°ì—´
            y: íƒ€ê²Ÿ ë°°ì—´
            threshold: ë¶„ë¥˜ ì„ê³„ê°’ (Noneì´ë©´ Youden Index ì‚¬ìš©)
            use_youden: thresholdê°€ Noneì¼ ë•Œ Youden Index ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        # ì˜ˆì¸¡
        y_prob = self.predict_proba(X)
        
        # Threshold ê²°ì •
        if threshold is None:
            if use_youden:
                threshold = self.find_optimal_threshold(X, y)
            else:
                threshold = 0.5
        
        y_pred = (y_prob >= threshold).astype(int)
        
        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        results = {
            'auroc': roc_auc_score(y, y_prob),
            'auprc': average_precision_score(y, y_prob),
            'threshold': threshold,
            'accuracy': accuracy_score(y, y_pred),
            'precision': precision_score(y, y_pred, zero_division=0),
            'recall': recall_score(y, y_pred, zero_division=0),  # Sensitivity
            'f1': f1_score(y, y_pred, zero_division=0),
            'brier_score': brier_score_loss(y, y_prob)
        }
        
        # Confusion Matrix
        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()
        results['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0
        results['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0
        results['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0
        results['ppv'] = tp / (tp + fp) if (tp + fp) > 0 else 0
        
        if self.youden_index is not None:
            results['youden_index'] = self.youden_index
        
        # ì €ì¥
        self.results = results
        
        return results
    
    def print_results(self) -> None:
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸ“Š {self.model_name} í‰ê°€ ê²°ê³¼")
        print("=" * 50)
        print(f"  AUROC (C-statistic): {self.results['auroc']:.4f}")
        print(f"  AUPRC:               {self.results['auprc']:.4f}")
        if 'threshold' in self.results:
            print(f"  Threshold (Youden):  {self.results['threshold']:.4f}")
        if 'youden_index' in self.results:
            print(f"  Youden Index:        {self.results['youden_index']:.4f}")
        print("-" * 50)
        print(f"  Accuracy:            {self.results['accuracy']:.4f}")
        print(f"  Sensitivity (Recall):{self.results.get('sensitivity', self.results.get('recall', 0)):.4f}")
        print(f"  Specificity:         {self.results['specificity']:.4f}")
        print(f"  PPV (Precision):     {self.results['ppv']:.4f}")
        print(f"  NPV:                 {self.results['npv']:.4f}")
        print(f"  F1 Score:            {self.results['f1']:.4f}")
        print(f"  Brier Score:         {self.results['brier_score']:.4f}")
    
    def plot_roc_curve(
        self,
        X: np.ndarray,
        y: np.ndarray,
        save_path: str = None,
        show_optimal_threshold: bool = True
    ) -> plt.Figure:
        """
        ROC Curve ì‹œê°í™”
        
        Args:
            X: íŠ¹ì„± ë°°ì—´
            y: íƒ€ê²Ÿ ë°°ì—´
            save_path: ì €ì¥ ê²½ë¡œ
            show_optimal_threshold: Youden Index ê¸°ë°˜ ìµœì ì  í‘œì‹œ ì—¬ë¶€
        """
        y_prob = self.predict_proba(X)
        fpr, tpr, thresholds = roc_curve(y, y_prob)
        auroc = roc_auc_score(y, y_prob)
        
        fig, ax = plt.subplots(figsize=(8, 8))
        ax.plot(fpr, tpr, 'b-', lw=2, label=f'{self.model_name} (AUROC = {auroc:.3f})')
        ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Random (AUROC = 0.500)')
        ax.fill_between(fpr, tpr, alpha=0.3)
        
        # ìµœì  threshold ì§€ì  í‘œì‹œ (Youden Index)
        if show_optimal_threshold:
            youden = tpr - fpr
            optimal_idx = np.argmax(youden)
            optimal_fpr = fpr[optimal_idx]
            optimal_tpr = tpr[optimal_idx]
            optimal_threshold = thresholds[optimal_idx]
            
            ax.scatter([optimal_fpr], [optimal_tpr], marker='o', s=100, c='red', 
                      zorder=5, label=f'Optimal (Youden)\nThreshold = {optimal_threshold:.3f}')
            ax.annotate(f'Sens={optimal_tpr:.2f}\nSpec={1-optimal_fpr:.2f}',
                       xy=(optimal_fpr, optimal_tpr), xytext=(optimal_fpr + 0.1, optimal_tpr - 0.1),
                       fontsize=10, ha='left',
                       arrowprops=dict(arrowstyle='->', color='red', lw=1.5))
        
        ax.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=12)
        ax.set_ylabel('True Positive Rate (Sensitivity)', fontsize=12)
        ax.set_title('ROC Curve', fontsize=14)
        ax.legend(loc='lower right')
        ax.set_xlim([0, 1])
        ax.set_ylim([0, 1])
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            _save_figure(fig, save_path)
        
        return fig
    
    def plot_pr_curve(
        self,
        X: np.ndarray,
        y: np.ndarray,
        save_path: str = None
    ) -> plt.Figure:
        """Precision-Recall Curve ì‹œê°í™”"""
        y_prob = self.predict_proba(X)
        precision, recall, thresholds = precision_recall_curve(y, y_prob)
        auprc = average_precision_score(y, y_prob)
        
        # ê¸°ì¤€ì„  (no skill)
        baseline = y.mean()
        
        fig, ax = plt.subplots(figsize=(8, 8))
        ax.plot(recall, precision, 'b-', lw=2, label=f'{self.model_name} (AUPRC = {auprc:.3f})')
        ax.axhline(y=baseline, color='k', linestyle='--', lw=1, label=f'Baseline (AUPRC = {baseline:.3f})')
        ax.fill_between(recall, precision, alpha=0.3)
        
        ax.set_xlabel('Recall (Sensitivity)', fontsize=12)
        ax.set_ylabel('Precision (PPV)', fontsize=12)
        ax.set_title('Precision-Recall Curve', fontsize=14)
        ax.legend(loc='upper right')
        ax.set_xlim([0, 1])
        ax.set_ylim([0, 1])
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            _save_figure(fig, save_path)
        
        return fig
    
    def plot_calibration_curve(
        self,
        X: np.ndarray,
        y: np.ndarray,
        n_bins: int = 10,
        save_path: str = None
    ) -> plt.Figure:
        """Calibration Curve ì‹œê°í™”"""
        y_prob = self.predict_proba(X)
        
        # Calibration curve
        fraction_of_positives, mean_predicted_value = calibration_curve(
            y, y_prob, n_bins=n_bins, strategy='uniform'
        )
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # Calibration plot
        ax1 = axes[0]
        ax1.plot([0, 1], [0, 1], 'k--', lw=1, label='Perfectly Calibrated')
        ax1.plot(mean_predicted_value, fraction_of_positives, 'b-o', lw=2, 
                label=f'{self.model_name}')
        ax1.set_xlabel('Mean Predicted Probability', fontsize=12)
        ax1.set_ylabel('Fraction of Positives', fontsize=12)
        ax1.set_title('Calibration Curve', fontsize=14)
        ax1.legend(loc='lower right')
        ax1.grid(True, alpha=0.3)
        
        # Histogram
        ax2 = axes[1]
        ax2.hist(y_prob, bins=50, edgecolor='black', alpha=0.7)
        ax2.set_xlabel('Predicted Probability', fontsize=12)
        ax2.set_ylabel('Count', fontsize=12)
        ax2.set_title('Distribution of Predicted Probabilities', fontsize=14)
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            _save_figure(fig, save_path)
        
        return fig
    
    def plot_confusion_matrix(
        self,
        X: np.ndarray,
        y: np.ndarray,
        threshold: float = 0.5,
        save_path: str = None
    ) -> plt.Figure:
        """Confusion Matrix ì‹œê°í™”"""
        y_prob = self.predict_proba(X)
        y_pred = (y_prob >= threshold).astype(int)
        cm = confusion_matrix(y, y_pred)
        
        fig, ax = plt.subplots(figsize=(8, 6))
        
        im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
        ax.figure.colorbar(im, ax=ax)
        
        ax.set(xticks=[0, 1], yticks=[0, 1],
               xticklabels=['Negative', 'Positive'],
               yticklabels=['Negative', 'Positive'],
               ylabel='True Label',
               xlabel='Predicted Label',
               title=f'Confusion Matrix (threshold={threshold})')
        
        # ê°’ í‘œì‹œ
        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, format(cm[i, j], 'd'),
                       ha="center", va="center",
                       color="white" if cm[i, j] > thresh else "black",
                       fontsize=16)
        
        plt.tight_layout()
        
        if save_path:
            _save_figure(fig, save_path)
        
        return fig
    
    def get_feature_importance(self) -> pd.DataFrame:
        """Feature Importance ë°˜í™˜"""
        importance = None
        
        # CatBoost ëª¨ë¸ ì²˜ë¦¬
        if hasattr(self.model, 'get_feature_importance'):
            try:
                importance = self.model.get_feature_importance()
            except Exception:
                pass
        
        # ì¼ë°˜ì ì¸ feature_importances_ ì†ì„±
        if importance is None and hasattr(self.model, 'feature_importances_'):
            importance = self.model.feature_importances_
        
        # ì„ í˜• ëª¨ë¸ì˜ coef_ ì†ì„±
        if importance is None and hasattr(self.model, 'coef_'):
            importance = np.abs(self.model.coef_[0])
        
        if importance is None:
            print("âš ï¸ í•´ë‹¹ ëª¨ë¸ì€ feature importanceë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
        
        # numpy arrayë¡œ ë³€í™˜
        importance = np.array(importance, dtype=float)
        
        # None ë˜ëŠ” NaN ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´
        importance = np.nan_to_num(importance, nan=0.0)
        
        feature_names = self.feature_names or [f'feature_{i}' for i in range(len(importance))]
        
        df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        return df
    
    def plot_feature_importance(
        self,
        top_n: int = 20,
        save_path: str = None
    ) -> plt.Figure:
        """Feature Importance ì‹œê°í™”"""
        df = self.get_feature_importance()
        if df is None:
            return None
        
        df_top = df.head(top_n)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        
        y_pos = np.arange(len(df_top))
        ax.barh(y_pos, df_top['importance'].values, align='center', color='steelblue')
        ax.set_yticks(y_pos)
        ax.set_yticklabels(df_top['feature'].values)
        ax.invert_yaxis()
        ax.set_xlabel('Importance', fontsize=12)
        ax.set_title(f'Feature Importance (Top {top_n})', fontsize=14)
        ax.grid(True, alpha=0.3, axis='x')
        
        plt.tight_layout()
        
        if save_path:
            _save_figure(fig, save_path)
        
        return fig


class SHAPAnalyzer:
    """SHAP ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        model: Any,
        feature_names: List[str] = None,
        model_type: str = 'tree',
        exclude_missing_indicator: bool = True
    ):
        """
        Args:
            model: í•™ìŠµëœ ëª¨ë¸
            feature_names: íŠ¹ì„± ì´ë¦„
            model_type: ëª¨ë¸ íƒ€ì… ('tree', 'linear', 'kernel')
            exclude_missing_indicator: SHAP ì‹œê°í™”ì—ì„œ missing indicator ì œì™¸ ì—¬ë¶€
        """
        if not HAS_SHAP:
            raise ImportError("SHAPê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        self.model = model
        self.feature_names = feature_names
        self.model_type = model_type
        self.exclude_missing_indicator = exclude_missing_indicator
        self.explainer = None
        self.shap_values = None
        self.expected_value = None  # base value ì €ì¥
    
    def _get_non_missing_indices(self) -> List[int]:
        """Missing indicatorê°€ ì•„ë‹Œ íŠ¹ì„±ì˜ ì¸ë±ìŠ¤ ë°˜í™˜"""
        if self.feature_names is None:
            return list(range(self.shap_values.shape[1]))
        
        return [
            i for i, name in enumerate(self.feature_names)
            if not name.endswith(MISSING_INDICATOR_SUFFIX)
        ]
    
    def _filter_missing_indicators(
        self,
        shap_values: np.ndarray,
        X: np.ndarray,
        feature_names: List[str]
    ) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        Missing indicator íŠ¹ì„± ì œì™¸
        
        Args:
            shap_values: SHAP values
            X: íŠ¹ì„± ë°ì´í„°
            feature_names: íŠ¹ì„± ì´ë¦„
            
        Returns:
            (filtered_shap_values, filtered_X, filtered_feature_names)
        """
        if not self.exclude_missing_indicator or feature_names is None:
            return shap_values, X, feature_names
        
        indices = self._get_non_missing_indices()
        
        if len(indices) == len(feature_names):
            # Missing indicatorê°€ ì—†ìŒ
            return shap_values, X, feature_names
        
        filtered_shap = shap_values[:, indices]
        filtered_X = X[:, indices]
        filtered_names = [feature_names[i] for i in indices]
        
        return filtered_shap, filtered_X, filtered_names
    
    def _extract_shap_values_for_positive_class(self, shap_values: Any) -> np.ndarray:
        """
        Binary classificationì—ì„œ positive classì˜ SHAP values ì¶”ì¶œ
        
        ë‹¤ì–‘í•œ í˜•íƒœì˜ SHAP output ì²˜ë¦¬:
        - list of arrays: [shap_class_0, shap_class_1] -> shap_class_1 ì„ íƒ
        - 3D array: (n_samples, n_features, n_classes) -> [:, :, 1] ì„ íƒ
        - 2D array: (n_samples, n_features) -> ê·¸ëŒ€ë¡œ ì‚¬ìš©
        """
        # Case 1: List of arrays (older SHAP versions, some models)
        if isinstance(shap_values, list):
            print(f"   SHAP values í˜•íƒœ: list (ê¸¸ì´={len(shap_values)})")
            if len(shap_values) == 2:
                return shap_values[1]  # positive class
            else:
                return shap_values[0]
        
        # Case 2: numpy array
        if isinstance(shap_values, np.ndarray):
            print(f"   SHAP values í˜•íƒœ: ndarray, shape={shap_values.shape}")
            
            # 3D array: (n_samples, n_features, n_classes)
            if shap_values.ndim == 3:
                if shap_values.shape[2] == 2:
                    return shap_values[:, :, 1]  # positive class
                else:
                    return shap_values[:, :, 0]
            
            # 2D array: (n_samples, n_features) - ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•íƒœ
            elif shap_values.ndim == 2:
                return shap_values
            
            # 1D array: ë‹¨ì¼ ìƒ˜í”Œ
            elif shap_values.ndim == 1:
                return shap_values.reshape(1, -1)
        
        # Case 3: shap.Explanation object (newer SHAP versions)
        if hasattr(shap_values, 'values'):
            return self._extract_shap_values_for_positive_class(shap_values.values)
        
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” SHAP values í˜•íƒœ: {type(shap_values)}")
    
    def _extract_expected_value(self, expected_value: Any) -> float:
        """
        expected_value (base value) ì¶”ì¶œ
        
        ë‹¤ì–‘í•œ í˜•íƒœ ì²˜ë¦¬:
        - scalar: ê·¸ëŒ€ë¡œ ì‚¬ìš©
        - array: positive class ì„ íƒ
        - list: positive class ì„ íƒ
        """
        if expected_value is None:
            return 0.0
        
        # numpy array
        if isinstance(expected_value, np.ndarray):
            if expected_value.ndim == 0:
                return float(expected_value)
            elif len(expected_value) == 2:
                return float(expected_value[1])  # positive class
            else:
                return float(expected_value[0])
        
        # list
        if isinstance(expected_value, list):
            if len(expected_value) == 2:
                return float(expected_value[1])  # positive class
            else:
                return float(expected_value[0])
        
        # scalar
        return float(expected_value)
    
    def compute_shap_values(
        self,
        X: np.ndarray,
        y: np.ndarray = None,
        background_data: np.ndarray = None,
        max_samples: int = 1000
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        SHAP ê°’ ê³„ì‚°
        
        Args:
            X: ì„¤ëª…í•  ë°ì´í„°
            y: íƒ€ê²Ÿ ë³€ìˆ˜ (stratified samplingìš©, Noneì´ë©´ ëœë¤ ìƒ˜í”Œë§)
            background_data: ë°°ê²½ ë°ì´í„° (kernel SHAPìš©)
            max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: 1000)
            
        Returns:
            (SHAP ê°’ ë°°ì—´, ìƒ˜í”Œ ë°ì´í„°)
        """
        print("\nğŸ” SHAP ê°’ ê³„ì‚° ì¤‘...")
        
        # Stratified sampling (outcome ë¹„ìœ¨ ìœ ì§€)
        if len(X) > max_samples:
            if y is not None:
                # Stratified sampling
                from sklearn.model_selection import StratifiedShuffleSplit
                sss = StratifiedShuffleSplit(n_splits=1, train_size=max_samples, random_state=1004)
                indices, _ = next(sss.split(X, y))
                X_sample = X[indices]
                print(f"   Stratified sampling: {len(X)} -> {len(X_sample)} samples")
                if y is not None:
                    original_ratio = y.mean()
                    sampled_ratio = y[indices].mean()
                    print(f"   Outcome ratio: original={original_ratio:.3f}, sampled={sampled_ratio:.3f}")
            else:
                # Random sampling (fallback)
                np.random.seed(1004)
                indices = np.random.choice(len(X), max_samples, replace=False)
                X_sample = X[indices]
                print(f"   Random sampling: {len(X)} -> {len(X_sample)} samples")
        else:
            X_sample = X.copy()
        
        # KernelExplainerëŠ” ëŠë¦¬ë¯€ë¡œ ìƒ˜í”Œ ìˆ˜ ì œí•œ
        kernel_max = 500
        if self.model_type == 'kernel' and len(X_sample) > kernel_max:
            np.random.seed(1004)
            k_indices = np.random.choice(len(X_sample), kernel_max, replace=False)
            X_sample = X_sample[k_indices]
            print(f"   KernelExplainerìš© ì¶”ê°€ ìƒ˜í”Œë§: -> {len(X_sample)} samples")
        
        print(f"   ìƒ˜í”Œ ìˆ˜: {len(X_sample)}")
        print(f"   ëª¨ë¸ íƒ€ì…: {self.model_type}")
        
        # Explainer ìƒì„± ë° SHAP ê°’ ê³„ì‚°
        # shap 0.32 í˜¸í™˜: TreeExplainer ì‹œë„ í›„ ì‹¤íŒ¨ ì‹œ KernelExplainer í´ë°±
        
        def _make_kernel_explainer(bg_data):
            """KernelExplainer ìƒì„± í—¬í¼"""
            def predict_proba_positive(x):
                proba = self.model.predict_proba(x)
                if proba.ndim == 2 and proba.shape[1] == 2:
                    return proba[:, 1]
                return proba
            return shap.KernelExplainer(predict_proba_positive, bg_data)
        
        try:
            if self.model_type == 'tree':
                try:
                    self.explainer = shap.TreeExplainer(self.model)
                    raw_shap_values = self.explainer.shap_values(X_sample)
                    print("   TreeExplainer ì‚¬ìš©")
                except Exception as te:
                    print(f"   âš ï¸ TreeExplainer ì‹¤íŒ¨: {te}")
                    print("   KernelExplainerë¡œ í´ë°±...")
                    bg_size = min(50, len(X_sample))
                    bg_indices = np.random.choice(len(X_sample), bg_size, replace=False)
                    self.explainer = _make_kernel_explainer(X_sample[bg_indices])
                    raw_shap_values = self.explainer.shap_values(X_sample, nsamples=100)
                
            elif self.model_type == 'linear':
                try:
                    self.explainer = shap.LinearExplainer(self.model, X_sample)
                    raw_shap_values = self.explainer.shap_values(X_sample)
                except Exception as le:
                    print(f"   âš ï¸ LinearExplainer ì‹¤íŒ¨: {le}")
                    bg_size = min(50, len(X_sample))
                    bg_indices = np.random.choice(len(X_sample), bg_size, replace=False)
                    self.explainer = _make_kernel_explainer(X_sample[bg_indices])
                    raw_shap_values = self.explainer.shap_values(X_sample, nsamples=100)
                
            else:  # kernel (ANN, ê¸°íƒ€ ëª¨ë¸)
                print("   KernelExplainer ì‚¬ìš©...")
                if background_data is None:
                    bg_size = min(50, len(X_sample))
                    bg_indices = np.random.choice(len(X_sample), bg_size, replace=False)
                    background_data = X_sample[bg_indices]
                
                self.explainer = _make_kernel_explainer(background_data)
                raw_shap_values = self.explainer.shap_values(X_sample, nsamples=200)
                
        except Exception as e:
            print(f"   âš ï¸ {self.model_type} explainer ì‹¤íŒ¨, KernelExplainerë¡œ í´ë°±: {e}")
            bg_size = min(50, len(X_sample))
            bg_indices = np.random.choice(len(X_sample), bg_size, replace=False)
            self.explainer = _make_kernel_explainer(X_sample[bg_indices])
            raw_shap_values = self.explainer.shap_values(X_sample, nsamples=100)
        
        # SHAP valuesë¥¼ 2D ë°°ì—´ë¡œ ë³€í™˜ (positive class)
        self.shap_values = self._extract_shap_values_for_positive_class(raw_shap_values)
        
        # Expected value ì¶”ì¶œ
        if hasattr(self.explainer, 'expected_value'):
            self.expected_value = self._extract_expected_value(self.explainer.expected_value)
        else:
            self.expected_value = 0.0
        
        print(f"âœ… SHAP ê°’ ê³„ì‚° ì™„ë£Œ:")
        print(f"   shape: {self.shap_values.shape}")
        print(f"   expected_value (base): {self.expected_value:.4f}")
        
        return self.shap_values, X_sample
    
    def plot_summary(
        self,
        X: np.ndarray,
        save_path: str = None,
        max_display: int = 20
    ) -> plt.Figure:
        """
        SHAP Summary Plot (Beeswarm Plot)
        
        Note: exclude_missing_indicator=Trueì¸ ê²½ìš° Missing indicator íŠ¹ì„± ì œì™¸
        """
        if self.shap_values is None:
            self.compute_shap_values(X)
        
        # Missing indicator ì œì™¸ ì²˜ë¦¬
        shap_vals, X_plot, feat_names = self._filter_missing_indicators(
            self.shap_values, X, self.feature_names
        )
        
        if self.exclude_missing_indicator and feat_names != self.feature_names:
            print(f"   ğŸ“Š Missing indicator ì œì™¸: {len(self.feature_names)} -> {len(feat_names)} íŠ¹ì„±")
        
        fig = plt.figure(figsize=(10, 10))
        shap.summary_plot(
            shap_vals, X_plot,
            feature_names=feat_names,
            max_display=max_display,
            show=False
        )
        
        # colorbar í¬ê¸° ì¡°ì • - Feature Value barê°€ ì˜ ë³´ì´ë„ë¡
        for cb_ax in fig.get_axes():
            # colorbar axesëŠ” ë³´í†µ ë§¤ìš° ì¢ì€ widthë¥¼ ê°€ì§
            pos = cb_ax.get_position()
            if pos.width < 0.05 and pos.width < pos.height * 0.3:
                # colorbar axesë¡œ íŒë‹¨ â†’ ë„ˆë¹„ë¥¼ í‚¤ìš°ê³  ìœ„ì¹˜ ì¡°ì •
                cb_ax.set_position([pos.x0 + 0.02, pos.y0, 0.02, pos.height])
                cb_ax.tick_params(labelsize=10)
        
        plt.tight_layout(rect=[0, 0, 0.92, 1], pad=1.0)
        
        if save_path:
            _save_figure(fig, save_path, pad_inches=0.3)
        
        return fig
    
    def plot_bar(
        self,
        save_path: str = None,
        max_display: int = 20
    ) -> plt.Figure:
        """
        SHAP Bar Plot (Mean absolute SHAP values)
        
        Note: exclude_missing_indicator=Trueì¸ ê²½ìš° Missing indicator íŠ¹ì„± ì œì™¸
        """
        if self.shap_values is None:
            raise ValueError("ë¨¼ì € compute_shap_values()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # Missing indicator ì œì™¸ ì²˜ë¦¬
        dummy_X = np.zeros_like(self.shap_values)
        shap_vals, _, feat_names = self._filter_missing_indicators(
            self.shap_values, dummy_X, self.feature_names
        )
        
        fig = plt.figure(figsize=(10, 10))
        
        # SHAP ê¸°ë³¸ bar plot ì‚¬ìš©
        shap.summary_plot(
            shap_vals,
            feature_names=feat_names,
            plot_type="bar",
            max_display=max_display,
            show=False
        )
        
        # 1:1 ë¹„ìœ¨ ë§ì¶”ê¸°
        fig.set_size_inches(10, 10)
        plt.tight_layout(pad=1.0)
        
        if save_path:
            _save_figure(fig, save_path, bbox_inches=None, pad_inches=0.3)
        
        return fig
    
    # Note: waterfall and dependence plots removed for shap 0.32 compatibility


def evaluate_model(
    model_path: str,
    data_dir: str = '../data/processed',
    output_dir: str = '../results',
    model_name: str = None
) -> Dict:
    """
    ëª¨ë¸ í‰ê°€ ì‹¤í–‰
    
    Args:
        model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        data_dir: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        model_name: ëª¨ë¸ ì´ë¦„
        
    Returns:
        í‰ê°€ ê²°ê³¼
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
    if model_name is None:
        model_name = os.path.basename(model_path).replace('_best_model', '').replace('_model', '')
        model_name = model_name.replace('.pkl', '').replace('.json', '').replace('.txt', '').replace('.cbm', '')
    
    print("=" * 60)
    print(f"ëª¨ë¸ í‰ê°€: {model_name}")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë“œ (ëª¨ë“  ëª¨ë¸ pklë¡œ í†µì¼)
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    # ë°ì´í„° ë¡œë“œ
    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))
    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))
    
    with open(os.path.join(data_dir, 'feature_names.txt'), 'r') as f:
        feature_names = f.read().strip().split('\n')
    
    print(f"ğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: {X_test.shape}")
    
    # í‰ê°€
    evaluator = ModelEvaluator(model, feature_names, model_name)
    results = evaluator.evaluate(X_test, y_test)
    evaluator.print_results()
    
    # ì‹œê°í™”
    model_output_dir = os.path.join(output_dir, model_name)
    os.makedirs(model_output_dir, exist_ok=True)
    
    evaluator.plot_roc_curve(X_test, y_test, 
                            save_path=os.path.join(model_output_dir, 'roc_curve.png'))
    evaluator.plot_pr_curve(X_test, y_test,
                           save_path=os.path.join(model_output_dir, 'pr_curve.png'))
    evaluator.plot_calibration_curve(X_test, y_test,
                                    save_path=os.path.join(model_output_dir, 'calibration_curve.png'))
    evaluator.plot_confusion_matrix(X_test, y_test,
                                   save_path=os.path.join(model_output_dir, 'confusion_matrix.png'))
    evaluator.plot_feature_importance(save_path=os.path.join(model_output_dir, 'feature_importance.png'))
    
    # ê²°ê³¼ ì €ì¥ (numpy íƒ€ì…ì„ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜)
    with open(os.path.join(model_output_dir, 'metrics.json'), 'w') as f:
        json.dump(convert_to_serializable(results), f, indent=2)
    
    return results


def _detect_model_type(model: Any) -> str:
    """
    ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
    
    Returns:
        'tree', 'linear', 'kernel' ì¤‘ í•˜ë‚˜
    """
    model_class_name = type(model).__name__.lower()
    
    # Tree-based models
    tree_models = [
        'xgbclassifier', 'xgbregressor', 'xgboost',
        'lgbmclassifier', 'lgbmregressor', 'lightgbm', 'booster',
        'catboostclassifier', 'catboostregressor', 'catboost',
        'randomforestclassifier', 'randomforestregressor',
        'gradientboostingclassifier', 'gradientboostingregressor',
        'decisiontreeclassifier', 'decisiontreeregressor',
        'extratreesclassifier', 'extratreesregressor'
    ]
    
    # Linear models
    linear_models = [
        'logisticregression', 'linearregression',
        'ridge', 'lasso', 'elasticnet',
        'sgdclassifier', 'sgdregressor'
    ]
    
    for tree_model in tree_models:
        if tree_model in model_class_name:
            return 'tree'
    
    for linear_model in linear_models:
        if linear_model in model_class_name:
            return 'linear'
    
    # Default to kernel (for ANN, SVM, etc.)
    return 'kernel'


def run_shap_analysis(
    model_path: str,
    data_dir: str = '../data/processed',
    output_dir: str = '../results',
    model_name: str = None,
    model_type: str = None,  # Noneì´ë©´ ìë™ ê°ì§€
    exclude_missing_indicator: bool = True
) -> None:
    """
    SHAP ë¶„ì„ ì‹¤í–‰
    
    Args:
        model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        data_dir: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        model_name: ëª¨ë¸ ì´ë¦„
        model_type: ëª¨ë¸ íƒ€ì… ('tree', 'linear', 'kernel'), Noneì´ë©´ ìë™ ê°ì§€
        exclude_missing_indicator: SHAP ì‹œê°í™”ì—ì„œ missing indicator ì œì™¸ ì—¬ë¶€
    """
    if not HAS_SHAP:
        print("âš ï¸ SHAPê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    # ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
    if model_name is None:
        model_name = os.path.basename(model_path).replace('_best_model', '').replace('_model', '')
        model_name = model_name.replace('.pkl', '').replace('.json', '').replace('.txt', '').replace('.cbm', '')
    
    print("\n" + "=" * 60)
    print(f"SHAP ë¶„ì„: {model_name}")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë“œ (ëª¨ë“  ëª¨ë¸ pklë¡œ í†µì¼)
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    # ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
    if model_type is None:
        model_type = _detect_model_type(model)
        print(f"ğŸ“Š ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€: {model_type}")
    
    # ë°ì´í„° ë¡œë“œ
    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))
    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))
    
    with open(os.path.join(data_dir, 'feature_names.txt'), 'r') as f:
        feature_names = f.read().strip().split('\n')
    
    # SHAP ë¶„ì„
    print(f"ğŸ“Š Missing indicator ì œì™¸: {exclude_missing_indicator}")
    analyzer = SHAPAnalyzer(
        model, feature_names, model_type,
        exclude_missing_indicator=exclude_missing_indicator
    )
    shap_values, X_sample = analyzer.compute_shap_values(X_test, y=y_test, max_samples=1000)
    
    # ì‹œê°í™”
    model_output_dir = os.path.join(output_dir, model_name)
    os.makedirs(model_output_dir, exist_ok=True)
    
    analyzer.plot_summary(X_sample, save_path=os.path.join(model_output_dir, 'shap_summary.png'))
    analyzer.plot_bar(save_path=os.path.join(model_output_dir, 'shap_bar.png'))
    
    print(f"\nâœ… SHAP ë¶„ì„ ì™„ë£Œ: {model_output_dir}/")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ë‹¹ë‡¨ë³‘ ì˜ˆì¸¡ ëª¨ë¸ í‰ê°€')
    parser.add_argument('--model', type=str, required=True,
                        help='ëª¨ë¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--data-dir', type=str, default='../data/processed',
                        help='ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', type=str, default='../results',
                        help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--shap', action='store_true',
                        help='SHAP ë¶„ì„ ì‹¤í–‰')
    parser.add_argument('--model-type', type=str, default=None,
                        choices=['tree', 'linear', 'kernel'],
                        help='SHAP explainer íƒ€ì… (ê¸°ë³¸: ìë™ ê°ì§€)')
    parser.add_argument('--include-missing-indicator', action='store_true',
                        help='SHAP ì‹œê°í™”ì— missing indicator íŠ¹ì„± í¬í•¨ (ê¸°ë³¸: ì œì™¸)')
    
    args = parser.parse_args()
    
    # ëª¨ë¸ í‰ê°€
    results = evaluate_model(
        model_path=args.model,
        data_dir=args.data_dir,
        output_dir=args.output
    )
    
    # SHAP ë¶„ì„
    if args.shap:
        run_shap_analysis(
            model_path=args.model,
            data_dir=args.data_dir,
            output_dir=args.output,
            model_type=args.model_type,
            exclude_missing_indicator=not args.include_missing_indicator
        )
    
    print("\nâœ… í‰ê°€ ì™„ë£Œ!")


if __name__ == '__main__':
    main()
