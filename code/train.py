"""
ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- Logistic Regression, XGBoost, LightGBM ì§€ì›
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Optuna)
- Cross Validation
"""

import os
import numpy as np
import pandas as pd
import pickle
import json
from datetime import datetime
from typing import Dict, Any, Tuple, Optional

# ëª¨ë¸
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold

# íì‡„ë§ì—ì„œ XGBoost, LightGBMì´ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´ ì‚¬ìš©
try:
    import xgboost as xgb
    HAS_XGB = True
except ImportError:
    HAS_XGB = False
    print("âš ï¸ XGBoost ë¯¸ì„¤ì¹˜ - XGBoost ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€")

try:
    import lightgbm as lgb
    HAS_LGB = True
except ImportError:
    HAS_LGB = False
    print("âš ï¸ LightGBM ë¯¸ì„¤ì¹˜ - LightGBM ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€")

# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
try:
    import optuna
    from optuna.samplers import TPESampler
    HAS_OPTUNA = True
except ImportError:
    HAS_OPTUNA = False
    print("âš ï¸ Optuna ë¯¸ì„¤ì¹˜ - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¶ˆê°€")


class DiabetesTrainer:
    """ë‹¹ë‡¨ë³‘ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        model_type: str = 'xgboost',
        random_state: int = 1004,
        n_jobs: int = -1
    ):
        """
        Args:
            model_type: ëª¨ë¸ íƒ€ì… ('logistic', 'rf', 'gbdt', 'xgboost', 'lightgbm')
            random_state: ëœë¤ ì‹œë“œ
            n_jobs: ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜
        """
        self.model_type = model_type
        self.random_state = random_state
        self.n_jobs = n_jobs
        self.model = None
        self.best_params = None
        self.cv_scores = None
        
    def get_default_model(self, params: Dict = None) -> Any:
        """ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜"""
        params = params or {}
        
        if self.model_type == 'logistic':
            return LogisticRegression(
                random_state=self.random_state,
                max_iter=1000,
                n_jobs=self.n_jobs,
                **params
            )
        
        elif self.model_type == 'rf':
            return RandomForestClassifier(
                random_state=self.random_state,
                n_jobs=self.n_jobs,
                **params
            )
        
        elif self.model_type == 'gbdt':
            return GradientBoostingClassifier(
                random_state=self.random_state,
                **params
            )
        
        elif self.model_type == 'xgboost':
            if not HAS_XGB:
                raise ImportError("XGBoostê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return xgb.XGBClassifier(
                random_state=self.random_state,
                n_jobs=self.n_jobs,
                use_label_encoder=False,
                eval_metric='logloss',
                **params
            )
        
        elif self.model_type == 'lightgbm':
            if not HAS_LGB:
                raise ImportError("LightGBMì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return lgb.LGBMClassifier(
                random_state=self.random_state,
                n_jobs=self.n_jobs,
                verbose=-1,
                **params
            )
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
    
    def get_param_space(self, trial) -> Dict:
        """Optunaìš© í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„"""
        
        if self.model_type == 'logistic':
            return {
                'C': trial.suggest_float('C', 0.001, 100, log=True),
                'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),
                'solver': 'saga'
            }
        
        elif self.model_type == 'rf':
            return {
                'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                'max_depth': trial.suggest_int('max_depth', 3, 15),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
            }
        
        elif self.model_type == 'gbdt':
            return {
                'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            }
        
        elif self.model_type == 'xgboost':
            return {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10, log=True),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            }
        
        elif self.model_type == 'lightgbm':
            return {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 15),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'num_leaves': trial.suggest_int('num_leaves', 15, 127),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10, log=True),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),
            }
        
        return {}
    
    def optimize_hyperparameters(
        self,
        X: np.ndarray,
        y: np.ndarray,
        n_trials: int = 50,
        cv: int = 5,
        scoring: str = 'roc_auc',
        timeout: int = None
    ) -> Dict:
        """
        Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        
        Args:
            X: íŠ¹ì„± ë°°ì—´
            y: íƒ€ê²Ÿ ë°°ì—´
            n_trials: ì‹œë„ íšŸìˆ˜
            cv: Cross-validation fold ìˆ˜
            scoring: í‰ê°€ ì§€í‘œ
            timeout: ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
            
        Returns:
            ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
        """
        if not HAS_OPTUNA:
            print("âš ï¸ Optunaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return {}
        
        def objective(trial):
            params = self.get_param_space(trial)
            model = self.get_default_model(params)
            
            skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=self.random_state)
            scores = cross_val_score(model, X, y, cv=skf, scoring=scoring, n_jobs=self.n_jobs)
            
            return scores.mean()
        
        print(f"\nğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ ({self.model_type})")
        print(f"   ì‹œë„ íšŸìˆ˜: {n_trials}, CV: {cv}-fold, ì§€í‘œ: {scoring}")
        
        # Optuna study ìƒì„±
        sampler = TPESampler(seed=self.random_state)
        study = optuna.create_study(direction='maximize', sampler=sampler)
        
        # verbosity ì„¤ì •
        optuna.logging.set_verbosity(optuna.logging.WARNING)
        
        study.optimize(
            objective,
            n_trials=n_trials,
            timeout=timeout,
            show_progress_bar=True
        )
        
        self.best_params = study.best_params
        print(f"\nâœ… ìµœì í™” ì™„ë£Œ!")
        print(f"   ìµœê³  ì ìˆ˜: {study.best_value:.4f}")
        print(f"   ìµœì  íŒŒë¼ë¯¸í„°: {self.best_params}")
        
        return self.best_params
    
    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray = None,
        y_val: np.ndarray = None,
        params: Dict = None,
        early_stopping_rounds: int = 50
    ) -> Any:
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            X_train: í›ˆë ¨ íŠ¹ì„±
            y_train: í›ˆë ¨ íƒ€ê²Ÿ
            X_val: ê²€ì¦ íŠ¹ì„± (early stoppingìš©)
            y_val: ê²€ì¦ íƒ€ê²Ÿ
            params: í•˜ì´í¼íŒŒë¼ë¯¸í„° (Noneì´ë©´ ê¸°ë³¸ê°’ ë˜ëŠ” ìµœì í™”ëœ ê°’ ì‚¬ìš©)
            early_stopping_rounds: Early stopping ë¼ìš´ë“œ ìˆ˜
            
        Returns:
            í•™ìŠµëœ ëª¨ë¸
        """
        # íŒŒë¼ë¯¸í„° ì„¤ì •
        if params is None:
            params = self.best_params or {}
        
        print(f"\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ({self.model_type})")
        
        # ëª¨ë¸ ìƒì„±
        self.model = self.get_default_model(params)
        
        # í•™ìŠµ
        if self.model_type in ['xgboost', 'lightgbm'] and X_val is not None:
            # Early stopping ì‚¬ìš©
            if self.model_type == 'xgboost':
                self.model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    verbose=False
                )
            elif self.model_type == 'lightgbm':
                self.model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    callbacks=[lgb.early_stopping(early_stopping_rounds, verbose=False)]
                )
        else:
            self.model.fit(X_train, y_train)
        
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        
        return self.model
    
    def cross_validate(
        self,
        X: np.ndarray,
        y: np.ndarray,
        cv: int = 5,
        scoring: str = 'roc_auc'
    ) -> Dict:
        """
        Cross-validation ìˆ˜í–‰
        
        Args:
            X: íŠ¹ì„± ë°°ì—´
            y: íƒ€ê²Ÿ ë°°ì—´
            cv: fold ìˆ˜
            scoring: í‰ê°€ ì§€í‘œ
            
        Returns:
            CV ê²°ê³¼
        """
        print(f"\nğŸ“Š Cross-Validation ({cv}-fold, {scoring})")
        
        params = self.best_params or {}
        model = self.get_default_model(params)
        
        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        scores = cross_val_score(model, X, y, cv=skf, scoring=scoring, n_jobs=self.n_jobs)
        
        self.cv_scores = {
            'scores': scores.tolist(),
            'mean': scores.mean(),
            'std': scores.std()
        }
        
        print(f"   ì ìˆ˜: {scores.round(4)}")
        print(f"   í‰ê· : {scores.mean():.4f} (Â±{scores.std():.4f})")
        
        return self.cv_scores
    
    def save_model(self, filepath: str) -> None:
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        if self.model_type == 'xgboost' and HAS_XGB:
            self.model.save_model(filepath.replace('.pkl', '.json'))
            print(f"âœ… XGBoost ëª¨ë¸ ì €ì¥: {filepath.replace('.pkl', '.json')}")
        elif self.model_type == 'lightgbm' and HAS_LGB:
            self.model.booster_.save_model(filepath.replace('.pkl', '.txt'))
            print(f"âœ… LightGBM ëª¨ë¸ ì €ì¥: {filepath.replace('.pkl', '.txt')}")
        else:
            with open(filepath, 'wb') as f:
                pickle.dump(self.model, f)
            print(f"âœ… ëª¨ë¸ ì €ì¥: {filepath}")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        meta = {
            'model_type': self.model_type,
            'best_params': self.best_params,
            'cv_scores': self.cv_scores,
            'train_date': datetime.now().isoformat()
        }
        meta_path = filepath.replace('.pkl', '_meta.json').replace('.json', '_meta.json').replace('.txt', '_meta.json')
        with open(meta_path, 'w') as f:
            json.dump(meta, f, indent=2)
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {meta_path}")
    
    def load_model(self, filepath: str) -> Any:
        """ëª¨ë¸ ë¡œë“œ"""
        if filepath.endswith('.json') and HAS_XGB:
            self.model = xgb.XGBClassifier()
            self.model.load_model(filepath)
        elif filepath.endswith('.txt') and HAS_LGB:
            self.model = lgb.Booster(model_file=filepath)
        else:
            with open(filepath, 'rb') as f:
                self.model = pickle.load(f)
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ: {filepath}")
        return self.model


def load_processed_data(data_dir: str = '../data/processed') -> Tuple:
    """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
    X_train = np.load(os.path.join(data_dir, 'X_train.npy'))
    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))
    
    X_val_path = os.path.join(data_dir, 'X_val.npy')
    if os.path.exists(X_val_path):
        X_val = np.load(X_val_path)
        y_val = np.load(os.path.join(data_dir, 'y_val.npy'))
    else:
        X_val, y_val = None, None
    
    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))
    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))
    
    # íŠ¹ì„± ì´ë¦„ ë¡œë“œ
    with open(os.path.join(data_dir, 'feature_names.txt'), 'r') as f:
        feature_names = f.read().strip().split('\n')
    
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    print(f"   - í›ˆë ¨: {X_train.shape}")
    if X_val is not None:
        print(f"   - ê²€ì¦: {X_val.shape}")
    print(f"   - í…ŒìŠ¤íŠ¸: {X_test.shape}")
    
    return X_train, y_train, X_val, y_val, X_test, y_test, feature_names


def train_all_models(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: np.ndarray,
    y_val: np.ndarray,
    output_dir: str = '../models',
    optimize: bool = True,
    n_trials: int = 30
) -> Dict[str, DiabetesTrainer]:
    """
    ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ
    
    Args:
        X_train, y_train: í›ˆë ¨ ë°ì´í„°
        X_val, y_val: ê²€ì¦ ë°ì´í„°
        output_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        optimize: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì—¬ë¶€
        n_trials: Optuna ì‹œë„ íšŸìˆ˜
        
    Returns:
        í•™ìŠµëœ ëª¨ë¸ë“¤
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
    model_types = ['logistic', 'rf', 'gbdt']
    if HAS_XGB:
        model_types.append('xgboost')
    if HAS_LGB:
        model_types.append('lightgbm')
    
    trainers = {}
    results = {}
    
    for model_type in model_types:
        print("\n" + "=" * 60)
        print(f"ğŸ¯ {model_type.upper()} í•™ìŠµ")
        print("=" * 60)
        
        trainer = DiabetesTrainer(model_type=model_type)
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        if optimize and HAS_OPTUNA:
            trainer.optimize_hyperparameters(
                X_train, y_train,
                n_trials=n_trials,
                cv=5
            )
        
        # í•™ìŠµ
        trainer.train(X_train, y_train, X_val, y_val)
        
        # Cross-validation
        cv_results = trainer.cross_validate(X_train, y_train)
        
        # ì €ì¥
        model_path = os.path.join(output_dir, f'{model_type}_model.pkl')
        trainer.save_model(model_path)
        
        trainers[model_type] = trainer
        results[model_type] = cv_results['mean']
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ëª¨ë¸ ë¹„êµ (CV AUROC)")
    print("=" * 60)
    for model_type, score in sorted(results.items(), key=lambda x: x[1], reverse=True):
        print(f"   {model_type:12s}: {score:.4f}")
    
    return trainers


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ë‹¹ë‡¨ë³‘ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--data-dir', type=str, default='../data/processed',
                        help='ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', type=str, default='../models',
                        help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--model', type=str, default='all',
                        choices=['logistic', 'rf', 'gbdt', 'xgboost', 'lightgbm', 'all'],
                        help='í•™ìŠµí•  ëª¨ë¸')
    parser.add_argument('--no-optimize', action='store_true',
                        help='í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë¹„í™œì„±í™”')
    parser.add_argument('--n-trials', type=int, default=30,
                        help='Optuna ì‹œë„ íšŸìˆ˜')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ë‹¹ë‡¨ë³‘ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë“œ
    X_train, y_train, X_val, y_val, X_test, y_test, feature_names = load_processed_data(args.data_dir)
    
    if args.model == 'all':
        # ëª¨ë“  ëª¨ë¸ í•™ìŠµ
        trainers = train_all_models(
            X_train, y_train, X_val, y_val,
            output_dir=args.output,
            optimize=not args.no_optimize,
            n_trials=args.n_trials
        )
    else:
        # ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ
        trainer = DiabetesTrainer(model_type=args.model)
        
        if not args.no_optimize and HAS_OPTUNA:
            trainer.optimize_hyperparameters(X_train, y_train, n_trials=args.n_trials)
        
        trainer.train(X_train, y_train, X_val, y_val)
        trainer.cross_validate(X_train, y_train)
        
        os.makedirs(args.output, exist_ok=True)
        trainer.save_model(os.path.join(args.output, f'{args.model}_model.pkl'))
    
    print("\nâœ… í•™ìŠµ ì™„ë£Œ!")


if __name__ == '__main__':
    main()
