"""
GridSearchCVë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- Decision Tree, Random Forest, XGBoost, CatBoost, ANN(MLP)
- Train/Test ë¶„í•  í›„ Trainìœ¼ë¡œ GridSearchCV, Testë¡œ ìµœì¢… í‰ê°€
- ìµœì  ëª¨ë¸ ì €ì¥ (SHAP ë¶„ì„ìš©)
"""

import os
import numpy as np
import pandas as pd
import pickle
import json
from datetime import datetime
from typing import Dict, Any, Tuple, List, Optional

# sklearn
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (
    roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, 
    f1_score, confusion_matrix, classification_report
)
from sklearn.preprocessing import StandardScaler

# XGBoost
try:
    import xgboost as xgb
    HAS_XGB = True
except ImportError:
    HAS_XGB = False
    print("âš ï¸ XGBoost ë¯¸ì„¤ì¹˜")

# CatBoost
try:
    from catboost import CatBoostClassifier
    HAS_CATBOOST = True
except ImportError:
    HAS_CATBOOST = False
    print("âš ï¸ CatBoost ë¯¸ì„¤ì¹˜")

import warnings
warnings.filterwarnings('ignore')


# =============================================================================
# í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
# =============================================================================

PARAM_GRIDS = {
    'decision_tree': {
        'max_depth': [3, 5, 7, 10, 15, None],
        'min_samples_split': [2, 5, 10, 20],
        'min_samples_leaf': [1, 2, 5, 10],
        'criterion': ['gini', 'entropy'],
        'class_weight': ['balanced', None]
    },
    
    'random_forest': {
        'n_estimators': [100, 200, 300],
        'max_depth': [5, 10, 15, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 5],
        'class_weight': ['balanced', None]
    },
    
    'xgboost': {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 5, 7, 10],
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'subsample': [0.7, 0.8, 0.9, 1.0],
        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
        'min_child_weight': [1, 3, 5]
    },
    
    'catboost': {
        'iterations': [100, 200, 300],
        'depth': [4, 6, 8, 10],
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'l2_leaf_reg': [1, 3, 5, 7]
    },
    
    'ann': {
        'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],
        'activation': ['relu', 'tanh'],
        'alpha': [0.0001, 0.001, 0.01],
        'learning_rate_init': [0.001, 0.01],
        'max_iter': [500]
    }
}

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ì¶•ì†Œ ê·¸ë¦¬ë“œ
PARAM_GRIDS_SMALL = {
    'decision_tree': {
        'max_depth': [5, 10, None],
        'min_samples_split': [2, 10],
        'min_samples_leaf': [1, 5],
        'criterion': ['gini'],
        'class_weight': ['balanced']
    },
    
    'random_forest': {
        'n_estimators': [100, 200],
        'max_depth': [10, None],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2],
        'class_weight': ['balanced']
    },
    
    'xgboost': {
        'n_estimators': [100, 200],
        'max_depth': [5, 7],
        'learning_rate': [0.05, 0.1],
        'subsample': [0.8],
        'colsample_bytree': [0.8],
        'min_child_weight': [1, 3]
    },
    
    'catboost': {
        'iterations': [100, 200],
        'depth': [6, 8],
        'learning_rate': [0.05, 0.1],
        'l2_leaf_reg': [3, 5]
    },
    
    'ann': {
        'hidden_layer_sizes': [(100,), (100, 50)],
        'activation': ['relu'],
        'alpha': [0.001],
        'learning_rate_init': [0.001],
        'max_iter': [300]
    }
}


class ModelTrainer:
    """GridSearchCVë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        random_state: int = 1004,
        cv: int = 5,
        scoring: str = 'roc_auc',
        n_jobs: int = -1,
        use_small_grid: bool = False
    ):
        """
        Args:
            random_state: ëœë¤ ì‹œë“œ
            cv: Cross-validation fold ìˆ˜
            scoring: í‰ê°€ ì§€í‘œ
            n_jobs: ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜
            use_small_grid: ì¶•ì†Œëœ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì‚¬ìš© ì—¬ë¶€
        """
        self.random_state = random_state
        self.cv = cv
        self.scoring = scoring
        self.n_jobs = n_jobs
        self.param_grids = PARAM_GRIDS_SMALL if use_small_grid else PARAM_GRIDS
        
        self.models = {}
        self.best_params = {}
        self.cv_results = {}
        self.test_results = {}
        
    def _get_base_model(self, model_name: str) -> Any:
        """ê¸°ë³¸ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if model_name == 'decision_tree':
            return DecisionTreeClassifier(random_state=self.random_state)
        
        elif model_name == 'random_forest':
            return RandomForestClassifier(random_state=self.random_state, n_jobs=self.n_jobs)
        
        elif model_name == 'xgboost':
            if not HAS_XGB:
                raise ImportError("XGBoostê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return xgb.XGBClassifier(
                random_state=self.random_state,
                n_jobs=self.n_jobs,
                use_label_encoder=False,
                eval_metric='logloss',
                verbosity=0
            )
        
        elif model_name == 'catboost':
            if not HAS_CATBOOST:
                raise ImportError("CatBoostê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return CatBoostClassifier(
                random_state=self.random_state,
                verbose=0,
                thread_count=self.n_jobs if self.n_jobs > 0 else -1
            )
        
        elif model_name == 'ann':
            return MLPClassifier(
                random_state=self.random_state,
                early_stopping=True,
                validation_fraction=0.1
            )
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")
    
    def train_model(
        self,
        model_name: str,
        X_train: np.ndarray,
        y_train: np.ndarray,
        verbose: int = 1
    ) -> Tuple[Any, Dict]:
        """
        ë‹¨ì¼ ëª¨ë¸ GridSearchCV í•™ìŠµ
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            X_train: í›ˆë ¨ íŠ¹ì„±
            y_train: í›ˆë ¨ íƒ€ê²Ÿ
            verbose: ì¶œë ¥ ìƒì„¸ë„
            
        Returns:
            (best_model, cv_results)
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¯ {model_name.upper()} í•™ìŠµ ì‹œì‘")
        print(f"{'='*60}")
        
        # ëª¨ë¸ê³¼ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
        base_model = self._get_base_model(model_name)
        param_grid = self.param_grids[model_name]
        
        # íŒŒë¼ë¯¸í„° ì¡°í•© ìˆ˜ ê³„ì‚°
        n_combinations = 1
        for values in param_grid.values():
            n_combinations *= len(values)
        print(f"ğŸ“Š íŒŒë¼ë¯¸í„° ì¡°í•© ìˆ˜: {n_combinations}")
        print(f"ğŸ“Š ì´ fit íšŸìˆ˜: {n_combinations * self.cv}")
        
        # GridSearchCV
        cv_splitter = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)
        
        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=param_grid,
            scoring=self.scoring,
            cv=cv_splitter,
            n_jobs=self.n_jobs,
            verbose=verbose,
            refit=True,
            return_train_score=True
        )
        
        print(f"\nğŸ” GridSearchCV ì§„í–‰ ì¤‘...")
        start_time = datetime.now()
        
        grid_search.fit(X_train, y_train)
        
        elapsed = datetime.now() - start_time
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed}")
        
        # ê²°ê³¼ ì €ì¥
        self.models[model_name] = grid_search.best_estimator_
        self.best_params[model_name] = grid_search.best_params_
        self.cv_results[model_name] = {
            'best_score': grid_search.best_score_,
            'best_params': grid_search.best_params_,
            'cv_results': {
                'mean_test_score': grid_search.cv_results_['mean_test_score'].tolist(),
                'std_test_score': grid_search.cv_results_['std_test_score'].tolist(),
                'mean_train_score': grid_search.cv_results_['mean_train_score'].tolist(),
                'params': [str(p) for p in grid_search.cv_results_['params']]
            }
        }
        
        print(f"\nâœ… ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
        print(f"âœ… ìµœì  CV ì ìˆ˜ ({self.scoring}): {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_, self.cv_results[model_name]
    
    def evaluate_model(
        self,
        model_name: str,
        X_test: np.ndarray,
        y_test: np.ndarray
    ) -> Dict[str, float]:
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            X_test: í…ŒìŠ¤íŠ¸ íŠ¹ì„±
            y_test: í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ
            
        Returns:
            í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        if model_name not in self.models:
            raise ValueError(f"ëª¨ë¸ '{model_name}'ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        model = self.models[model_name]
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]
        
        # í‰ê°€ ì§€í‘œ
        results = {
            'auroc': roc_auc_score(y_test, y_prob),
            'auprc': average_precision_score(y_test, y_prob),
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1': f1_score(y_test, y_pred, zero_division=0)
        }
        
        # Confusion matrix
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        results['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0
        results['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0
        
        self.test_results[model_name] = results
        
        return results
    
    def save_model(
        self,
        model_name: str,
        output_dir: str,
        feature_names: List[str] = None
    ) -> str:
        """
        í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            feature_names: íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if model_name not in self.models:
            raise ValueError(f"ëª¨ë¸ '{model_name}'ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        os.makedirs(output_dir, exist_ok=True)
        
        model = self.models[model_name]
        
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì €ì¥
        if model_name == 'xgboost':
            model_path = os.path.join(output_dir, f'{model_name}_best_model.json')
            model.save_model(model_path)
        elif model_name == 'catboost':
            model_path = os.path.join(output_dir, f'{model_name}_best_model.cbm')
            model.save_model(model_path)
        else:
            model_path = os.path.join(output_dir, f'{model_name}_best_model.pkl')
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥: {model_path}")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        meta = {
            'model_name': model_name,
            'best_params': self.best_params.get(model_name, {}),
            'cv_score': self.cv_results.get(model_name, {}).get('best_score'),
            'test_results': self.test_results.get(model_name, {}),
            'feature_names': feature_names,
            'train_date': datetime.now().isoformat(),
            'scoring': self.scoring,
            'cv_folds': self.cv
        }
        
        meta_path = os.path.join(output_dir, f'{model_name}_best_model_meta.json')
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(meta, f, indent=2, ensure_ascii=False)
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {meta_path}")
        
        return model_path
    
    def save_all_models(
        self,
        output_dir: str,
        feature_names: List[str] = None
    ) -> Dict[str, str]:
        """ëª¨ë“  í•™ìŠµëœ ëª¨ë¸ ì €ì¥"""
        paths = {}
        for model_name in self.models:
            paths[model_name] = self.save_model(model_name, output_dir, feature_names)
        return paths
    
    @staticmethod
    def load_model(model_path: str) -> Any:
        """
        ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
        
        Args:
            model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë¡œë“œëœ ëª¨ë¸
        """
        if model_path.endswith('.json'):
            # XGBoost
            model = xgb.XGBClassifier()
            model.load_model(model_path)
        elif model_path.endswith('.cbm'):
            # CatBoost
            model = CatBoostClassifier()
            model.load_model(model_path)
        elif model_path.endswith('.pkl'):
            # Pickle
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {model_path}")
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ: {model_path}")
        return model


def load_data(
    data_path: str,
    target_col: str = 'outA',
    test_size: float = 0.2,
    random_state: int = 1004
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, List[str]]:
    """
    ë°ì´í„° ë¡œë“œ ë° Train/Test ë¶„í• 
    
    Args:
        data_path: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬ ë˜ëŠ” CSV íŒŒì¼ ê²½ë¡œ
        target_col: íƒ€ê²Ÿ ë³€ìˆ˜
        test_size: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨
        random_state: ëœë¤ ì‹œë“œ
        
    Returns:
        (X_train, X_test, y_train, y_test, feature_names)
    """
    # ì „ì²˜ë¦¬ëœ numpy íŒŒì¼ ë¡œë“œ
    if os.path.isdir(data_path):
        X_train = np.load(os.path.join(data_path, 'X_train.npy'))
        y_train = np.load(os.path.join(data_path, 'y_train.npy'))
        X_test = np.load(os.path.join(data_path, 'X_test.npy'))
        y_test = np.load(os.path.join(data_path, 'y_test.npy'))
        
        # Validation ë°ì´í„°ê°€ ìˆìœ¼ë©´ trainì— í•©ì¹˜ê¸°
        X_val_path = os.path.join(data_path, 'X_val.npy')
        if os.path.exists(X_val_path):
            X_val = np.load(X_val_path)
            y_val = np.load(os.path.join(data_path, 'y_val.npy'))
            X_train = np.vstack([X_train, X_val])
            y_train = np.concatenate([y_train, y_val])
        
        # íŠ¹ì„± ì´ë¦„ ë¡œë“œ
        feature_names_path = os.path.join(data_path, 'feature_names.txt')
        if os.path.exists(feature_names_path):
            with open(feature_names_path, 'r') as f:
                feature_names = f.read().strip().split('\n')
        else:
            feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]
    
    else:
        raise ValueError(f"ë°ì´í„° ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
    
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    print(f"   - í›ˆë ¨: {X_train.shape}, ì–‘ì„±: {y_train.sum()} ({y_train.mean()*100:.1f}%)")
    print(f"   - í…ŒìŠ¤íŠ¸: {X_test.shape}, ì–‘ì„±: {y_test.sum()} ({y_test.mean()*100:.1f}%)")
    
    return X_train, X_test, y_train, y_test, feature_names


def train_all_models(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    feature_names: List[str],
    output_dir: str = '../models',
    model_list: List[str] = None,
    use_small_grid: bool = False,
    cv: int = 5,
    scoring: str = 'roc_auc',
    n_jobs: int = -1
) -> Tuple[ModelTrainer, pd.DataFrame]:
    """
    ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ
    
    Args:
        X_train, y_train: í›ˆë ¨ ë°ì´í„°
        X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        feature_names: íŠ¹ì„± ì´ë¦„
        output_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        model_list: í•™ìŠµí•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
        use_small_grid: ì¶•ì†Œ ê·¸ë¦¬ë“œ ì‚¬ìš© ì—¬ë¶€
        cv: CV fold ìˆ˜
        scoring: í‰ê°€ ì§€í‘œ
        n_jobs: ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜
        
    Returns:
        (trainer, results_df)
    """
    # ê¸°ë³¸ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
    if model_list is None:
        model_list = ['decision_tree', 'random_forest']
        if HAS_XGB:
            model_list.append('xgboost')
        if HAS_CATBOOST:
            model_list.append('catboost')
        model_list.append('ann')
    
    print("=" * 60)
    print("ğŸš€ GridSearchCV ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    print(f"ğŸ“Š í•™ìŠµ ëª¨ë¸: {model_list}")
    print(f"ğŸ“Š CV Folds: {cv}")
    print(f"ğŸ“Š í‰ê°€ ì§€í‘œ: {scoring}")
    print(f"ğŸ“Š íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ: {'ì¶•ì†Œ' if use_small_grid else 'ì „ì²´'}")
    
    # Trainer ìƒì„±
    trainer = ModelTrainer(
        cv=cv,
        scoring=scoring,
        n_jobs=n_jobs,
        use_small_grid=use_small_grid
    )
    
    # ê²°ê³¼ ì €ì¥
    results = []
    
    for model_name in model_list:
        try:
            # í•™ìŠµ
            trainer.train_model(model_name, X_train, y_train, verbose=1)
            
            # í‰ê°€
            test_metrics = trainer.evaluate_model(model_name, X_test, y_test)
            
            # ëª¨ë¸ ì €ì¥
            trainer.save_model(model_name, output_dir, feature_names)
            
            # ê²°ê³¼ ê¸°ë¡
            results.append({
                'model': model_name,
                'cv_score': trainer.cv_results[model_name]['best_score'],
                **test_metrics
            })
            
            print(f"\nğŸ“Š {model_name.upper()} í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            print(f"   AUROC: {test_metrics['auroc']:.4f}")
            print(f"   Accuracy: {test_metrics['accuracy']:.4f}")
            print(f"   Sensitivity: {test_metrics['recall']:.4f}")
            print(f"   Specificity: {test_metrics['specificity']:.4f}")
            
        except Exception as e:
            print(f"\nâŒ {model_name} í•™ìŠµ ì‹¤íŒ¨: {e}")
            continue
    
    # ê²°ê³¼ DataFrame
    results_df = pd.DataFrame(results)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š ëª¨ë¸ ë¹„êµ ê²°ê³¼")
    print("=" * 60)
    print(results_df.to_string(index=False))
    
    # ìµœê³  ëª¨ë¸
    if len(results_df) > 0:
        best_model = results_df.loc[results_df['auroc'].idxmax(), 'model']
        best_auroc = results_df['auroc'].max()
        print(f"\nğŸ† ìµœê³  ëª¨ë¸: {best_model} (Test AUROC: {best_auroc:.4f})")
    
    # ê²°ê³¼ ì €ì¥
    results_path = os.path.join(output_dir, 'model_comparison_results.csv')
    results_df.to_csv(results_path, index=False)
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {results_path}")
    
    return trainer, results_df


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='GridSearchCVë¥¼ ì‚¬ìš©í•œ ë‹¹ë‡¨ë³‘ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--data-dir', type=str, default='../data/processed',
                        help='ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', type=str, default='../models',
                        help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--models', type=str, nargs='+', 
                        default=['decision_tree', 'random_forest', 'xgboost', 'catboost', 'ann'],
                        help='í•™ìŠµí•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸')
    parser.add_argument('--small-grid', action='store_true',
                        help='ì¶•ì†Œëœ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)')
    parser.add_argument('--cv', type=int, default=5,
                        help='Cross-validation fold ìˆ˜')
    parser.add_argument('--scoring', type=str, default='roc_auc',
                        help='í‰ê°€ ì§€í‘œ')
    parser.add_argument('--n-jobs', type=int, default=-1,
                        help='ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ (-1: ëª¨ë“  ì½”ì–´)')
    
    args = parser.parse_args()
    
    # ë°ì´í„° ë¡œë“œ
    X_train, X_test, y_train, y_test, feature_names = load_data(args.data_dir)
    
    # ëª¨ë¸ í•™ìŠµ
    trainer, results_df = train_all_models(
        X_train, y_train, X_test, y_test, feature_names,
        output_dir=args.output,
        model_list=args.models,
        use_small_grid=args.small_grid,
        cv=args.cv,
        scoring=args.scoring,
        n_jobs=args.n_jobs
    )
    
    print("\nâœ… ëª¨ë“  í•™ìŠµ ì™„ë£Œ!")
    print(f"\nì €ì¥ëœ ëª¨ë¸ ìœ„ì¹˜: {args.output}/")
    print("SHAP ë¶„ì„ ì‹œ ëª¨ë¸ ë¡œë“œ ì˜ˆì‹œ:")
    print("  model = ModelTrainer.load_model('models/xgboost_best_model.json')")


if __name__ == '__main__':
    main()
