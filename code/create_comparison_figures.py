"""
ëª¨ë¸ ë¹„êµ ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸
- ROC Curve ë¹„êµ
- PR Curve ë¹„êµ
- Calibration Curve ë¹„êµ
- SHAP Beeswarm ë¹„êµ
- ë…¼ë¬¸ìš© Figure ìƒì„±
"""

import os
import numpy as np
import pandas as pd
import pickle
import json
import argparse
from typing import Dict, List, Any, Tuple
import warnings
warnings.filterwarnings('ignore')

from sklearn.metrics import (
    roc_curve, precision_recall_curve, 
    roc_auc_score, average_precision_score,
    brier_score_loss
)
from sklearn.calibration import calibration_curve

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# numpy í˜¸í™˜ì„± íŒ¨ì¹˜ (shap 0.32 + numpy>=1.24)
if not hasattr(np, 'int'):
    np.int = int
if not hasattr(np, 'float'):
    np.float = float
if not hasattr(np, 'bool'):
    np.bool = bool
if not hasattr(np, 'str'):
    np.str = str
if not hasattr(np, 'object'):
    np.object = object

# SHAP
try:
    import shap
    HAS_SHAP = True
except ImportError:
    HAS_SHAP = False
    print("âš ï¸ SHAP ë¯¸ì„¤ì¹˜ - SHAP ë¹„êµ Figure ìƒì„± ë¶ˆê°€")

# Missing Indicator ì ‘ë¯¸ì‚¬
MISSING_INDICATOR_SUFFIX = '_missing'

# í°íŠ¸ ì„¤ì •
def set_font():
    """ì‹œìŠ¤í…œì— ë§ëŠ” í°íŠ¸ ì„¤ì •"""
    import platform
    system = platform.system()
    
    if system == 'Darwin':
        font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'
        if os.path.exists(font_path):
            fm.fontManager.addfont(font_path)
            plt.rcParams['font.family'] = 'AppleGothic'
    elif system == 'Windows':
        plt.rcParams['font.family'] = 'Malgun Gothic'
    else:
        # Linux - fallback to DejaVu Sans
        plt.rcParams['font.family'] = 'DejaVu Sans'
    
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['font.size'] = 11
    plt.rcParams['axes.labelsize'] = 12
    plt.rcParams['axes.titlesize'] = 14
    plt.rcParams['legend.fontsize'] = 10
    plt.rcParams['xtick.labelsize'] = 10
    plt.rcParams['ytick.labelsize'] = 10

set_font()

# ëª¨ë¸ í‘œì‹œ ì´ë¦„
MODEL_DISPLAY_NAMES = {
    'decision_tree': 'Decision Tree',
    'random_forest': 'Random Forest',
    'xgboost': 'XGBoost',
    'catboost': 'CatBoost',
    'lightgbm': 'LightGBM',
    'ann': 'MLP',
    'logistic': 'Logistic Regression'
}

# ëª¨ë¸ë³„ ìƒ‰ìƒ
MODEL_COLORS = {
    'decision_tree': '#1f77b4',
    'random_forest': '#ff7f0e',
    'xgboost': '#2ca02c',
    'catboost': '#d62728',
    'lightgbm': '#9467bd',
    'ann': '#8c564b',
    'logistic': '#e377c2'
}

# ëª¨ë¸ë³„ ë¼ì¸ ìŠ¤íƒ€ì¼
MODEL_LINESTYLES = {
    'decision_tree': '-',
    'random_forest': '-',
    'xgboost': '-',
    'catboost': '-',
    'lightgbm': '-',
    'ann': '--',
    'logistic': '--'
}


def _save_figure(fig, save_path, dpi=500, bbox_inches='tight', pad_inches=0.1):
    """Figureë¥¼ png, tiff, pdf 3ì¢…ìœ¼ë¡œ ì €ì¥"""
    base, _ = os.path.splitext(save_path)
    for fmt in ['png', 'tiff', 'pdf']:
        out = f"{base}.{fmt}"
        fig.savefig(out, dpi=dpi, bbox_inches=bbox_inches, pad_inches=pad_inches, format=fmt)
    print(f"âœ… Figure ì €ì¥: {base}.{{png,tiff,pdf}}")


def load_model(model_path: str) -> Any:
    """ëª¨ë¸ ë¡œë“œ (ëª¨ë“  ëª¨ë¸ pklë¡œ í†µì¼)"""
    try:
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        return model
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {os.path.basename(model_path)} - {e}")
        return None


def find_models(models_dir: str) -> Dict[str, str]:
    """ëª¨ë¸ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ëª¨ë¸ ì°¾ê¸°"""
    models = {}
    
    patterns = [
        ('decision_tree', 'decision_tree_best_model.pkl'),
        ('random_forest', 'random_forest_best_model.pkl'),
        ('xgboost', 'xgboost_best_model.pkl'),
        ('lightgbm', 'lightgbm_best_model.pkl'),
        ('ann', 'ann_best_model.pkl'),
        ('logistic', 'logistic_best_model.pkl')
    ]
    
    for model_name, filename in patterns:
        model_path = os.path.join(models_dir, filename)
        if os.path.exists(model_path):
            models[model_name] = model_path
    
    return models


def get_predictions(model: Any, X: np.ndarray) -> np.ndarray:
    """ëª¨ë¸ ì˜ˆì¸¡ í™•ë¥  ë°˜í™˜"""
    if hasattr(model, 'predict_proba'):
        y_prob = model.predict_proba(X)
        if len(y_prob.shape) == 2 and y_prob.shape[1] == 2:
            return y_prob[:, 1]
        return y_prob
    elif hasattr(model, 'predict'):
        return model.predict(X)
    else:
        raise ValueError("ëª¨ë¸ì´ predict_proba ë˜ëŠ” predict ë©”ì„œë“œë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


def plot_roc_comparison(
    models_data: Dict[str, Tuple[np.ndarray, np.ndarray, float]],
    save_path: str = None,
    figsize: Tuple[int, int] = (8, 8)
) -> plt.Figure:
    """
    ROC Curve ë¹„êµ ì‹œê°í™”
    
    Args:
        models_data: {model_name: (fpr, tpr, auc)}
        save_path: ì €ì¥ ê²½ë¡œ
        figsize: Figure í¬ê¸°
    """
    fig, ax = plt.subplots(figsize=figsize)
    
    for model_name, (fpr, tpr, auc_score) in models_data.items():
        display_name = MODEL_DISPLAY_NAMES.get(model_name, model_name)
        color = MODEL_COLORS.get(model_name, 'gray')
        linestyle = MODEL_LINESTYLES.get(model_name, '-')
        
        ax.plot(fpr, tpr, 
                label=f'{display_name} (AUC = {auc_score:.3f})',
                color=color, 
                linestyle=linestyle,
                linewidth=2)
    
    # Diagonal line
    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Random (AUC = 0.500)')
    
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('1 - Specificity (False Positive Rate)')
    ax.set_ylabel('Sensitivity (True Positive Rate)')
    ax.set_title('ROC Curve Comparison')
    ax.legend(loc='lower right', framealpha=0.9)
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal')
    
    plt.tight_layout()
    
    if save_path:
        _save_figure(fig, save_path)
    
    return fig


def plot_pr_comparison(
    models_data: Dict[str, Tuple[np.ndarray, np.ndarray, float]],
    baseline: float = None,
    save_path: str = None,
    figsize: Tuple[int, int] = (8, 8)
) -> plt.Figure:
    """
    Precision-Recall Curve ë¹„êµ ì‹œê°í™”
    
    Args:
        models_data: {model_name: (recall, precision, ap)}
        baseline: ë² ì´ìŠ¤ë¼ì¸ (ì–‘ì„± í´ë˜ìŠ¤ ë¹„ìœ¨)
        save_path: ì €ì¥ ê²½ë¡œ
        figsize: Figure í¬ê¸°
    """
    fig, ax = plt.subplots(figsize=figsize)
    
    for model_name, (recall, precision, ap_score) in models_data.items():
        display_name = MODEL_DISPLAY_NAMES.get(model_name, model_name)
        color = MODEL_COLORS.get(model_name, 'gray')
        linestyle = MODEL_LINESTYLES.get(model_name, '-')
        
        ax.plot(recall, precision,
                label=f'{display_name} (AP = {ap_score:.3f})',
                color=color,
                linestyle=linestyle,
                linewidth=2)
    
    # Baseline
    if baseline is not None:
        ax.axhline(y=baseline, color='gray', linestyle='--', 
                   linewidth=1, alpha=0.5, label=f'Baseline ({baseline:.3f})')
    
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('Recall (Sensitivity)')
    ax.set_ylabel('Precision (PPV)')
    ax.set_title('Precision-Recall Curve Comparison')
    ax.legend(loc='upper right', framealpha=0.9)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        _save_figure(fig, save_path)
    
    return fig


def plot_calibration_comparison(
    models_data: Dict[str, Tuple[np.ndarray, np.ndarray, float]],
    save_path: str = None,
    figsize: Tuple[int, int] = (8, 8)
) -> plt.Figure:
    """
    Calibration Curve ë¹„êµ ì‹œê°í™”
    
    Args:
        models_data: {model_name: (prob_true, prob_pred, brier)}
        save_path: ì €ì¥ ê²½ë¡œ
        figsize: Figure í¬ê¸°
    """
    fig, ax = plt.subplots(figsize=figsize)
    
    for model_name, (prob_true, prob_pred, brier) in models_data.items():
        display_name = MODEL_DISPLAY_NAMES.get(model_name, model_name)
        color = MODEL_COLORS.get(model_name, 'gray')
        
        ax.plot(prob_pred, prob_true,
                marker='o',
                label=f'{display_name} (Brier = {brier:.3f})',
                color=color,
                linewidth=2,
                markersize=6)
    
    # Perfect calibration line
    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Perfect Calibration')
    
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.0])
    ax.set_xlabel('Mean Predicted Probability')
    ax.set_ylabel('Fraction of Positives')
    ax.set_title('Calibration Curve Comparison')
    ax.legend(loc='upper left', framealpha=0.9)
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal')
    
    plt.tight_layout()
    
    if save_path:
        _save_figure(fig, save_path)
    
    return fig


def plot_combined_comparison(
    roc_data: Dict[str, Tuple[np.ndarray, np.ndarray, float]],
    pr_data: Dict[str, Tuple[np.ndarray, np.ndarray, float]],
    cal_data: Dict[str, Tuple[np.ndarray, np.ndarray, float]],
    baseline: float = None,
    save_path: str = None,
    figsize: Tuple[int, int] = (16, 5)
) -> plt.Figure:
    """
    ROC, PR, Calibrationì„ í•˜ë‚˜ì˜ Figureë¡œ ê²°í•©
    
    Args:
        roc_data: ROC curve ë°ì´í„°
        pr_data: PR curve ë°ì´í„°
        cal_data: Calibration curve ë°ì´í„°
        baseline: PR curve ë² ì´ìŠ¤ë¼ì¸
        save_path: ì €ì¥ ê²½ë¡œ
        figsize: Figure í¬ê¸°
    """
    fig, axes = plt.subplots(1, 3, figsize=figsize)
    
    # (A) ROC Curve
    ax = axes[0]
    for model_name, (fpr, tpr, auc_score) in roc_data.items():
        display_name = MODEL_DISPLAY_NAMES.get(model_name, model_name)
        color = MODEL_COLORS.get(model_name, 'gray')
        linestyle = MODEL_LINESTYLES.get(model_name, '-')
        
        ax.plot(fpr, tpr,
                label=f'{display_name} ({auc_score:.3f})',
                color=color,
                linestyle=linestyle,
                linewidth=2)
    
    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('1 - Specificity')
    ax.set_ylabel('Sensitivity')
    ax.set_title('(A) ROC Curve')
    ax.legend(loc='lower right', fontsize=9, framealpha=0.9)
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal')
    
    # (B) PR Curve
    ax = axes[1]
    for model_name, (recall, precision, ap_score) in pr_data.items():
        display_name = MODEL_DISPLAY_NAMES.get(model_name, model_name)
        color = MODEL_COLORS.get(model_name, 'gray')
        linestyle = MODEL_LINESTYLES.get(model_name, '-')
        
        ax.plot(recall, precision,
                label=f'{display_name} ({ap_score:.3f})',
                color=color,
                linestyle=linestyle,
                linewidth=2)
    
    if baseline is not None:
        ax.axhline(y=baseline, color='gray', linestyle='--', linewidth=1, alpha=0.5)
    
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('Recall')
    ax.set_ylabel('Precision')
    ax.set_title('(B) Precision-Recall Curve')
    ax.legend(loc='upper right', fontsize=9, framealpha=0.9)
    ax.grid(True, alpha=0.3)
    
    # (C) Calibration Curve
    ax = axes[2]
    for model_name, (prob_true, prob_pred, brier) in cal_data.items():
        display_name = MODEL_DISPLAY_NAMES.get(model_name, model_name)
        color = MODEL_COLORS.get(model_name, 'gray')
        
        ax.plot(prob_pred, prob_true,
                marker='o',
                label=f'{display_name} ({brier:.3f})',
                color=color,
                linewidth=2,
                markersize=5)
    
    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.0])
    ax.set_xlabel('Mean Predicted Probability')
    ax.set_ylabel('Fraction of Positives')
    ax.set_title('(C) Calibration Curve')
    ax.legend(loc='upper left', fontsize=9, framealpha=0.9)
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal')
    
    plt.tight_layout()
    
    if save_path:
        _save_figure(fig, save_path)
    
    return fig


def _detect_model_type(model: Any) -> str:
    """ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€"""
    model_class_name = type(model).__name__.lower()
    
    tree_models = ['xgbclassifier', 'lgbmclassifier', 'randomforestclassifier', 
                   'decisiontreeclassifier', 'catboostclassifier', 'booster']
    
    for tree_model in tree_models:
        if tree_model in model_class_name:
            return 'tree'
    
    if 'mlp' in model_class_name or 'neural' in model_class_name:
        return 'kernel'
    
    return 'kernel'


def compute_shap_values(
    model: Any,
    X: np.ndarray,
    feature_names: List[str],
    y: np.ndarray = None,
    max_samples: int = 1000
) -> Tuple[np.ndarray, np.ndarray, List[str]]:
    """
    SHAP ê°’ ê³„ì‚°
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        X: ì…ë ¥ ë°ì´í„°
        feature_names: íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        y: íƒ€ê²Ÿ ë³€ìˆ˜ (stratified samplingìš©, Noneì´ë©´ ëœë¤ ìƒ˜í”Œë§)
        max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: 1000)
    
    Returns:
        (shap_values, X_sample, feature_names_filtered)
    """
    if not HAS_SHAP:
        raise ImportError("SHAPê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # Stratified sampling (outcome ë¹„ìœ¨ ìœ ì§€)
    if len(X) > max_samples:
        if y is not None:
            from sklearn.model_selection import StratifiedShuffleSplit
            sss = StratifiedShuffleSplit(n_splits=1, train_size=max_samples, random_state=1004)
            indices, _ = next(sss.split(X, y))
            X_sample = X[indices]
            print(f"      Stratified sampling: {len(X)} -> {len(X_sample)} (ratio: {y[indices].mean():.3f})")
        else:
            np.random.seed(1004)
            indices = np.random.choice(len(X), max_samples, replace=False)
            X_sample = X[indices]
            print(f"      Random sampling: {len(X)} -> {len(X_sample)}")
    else:
        X_sample = X
    
    # KernelExplainerëŠ” ëŠë¦¬ë¯€ë¡œ ìƒ˜í”Œ ìˆ˜ ì œí•œ
    model_type = _detect_model_type(model)
    kernel_max = 500
    if model_type == 'kernel' and len(X_sample) > kernel_max:
        np.random.seed(1004)
        k_indices = np.random.choice(len(X_sample), kernel_max, replace=False)
        X_sample = X_sample[k_indices]
        print(f"      KernelExplainerìš© ì¶”ê°€ ìƒ˜í”Œë§: -> {len(X_sample)} samples")
    
    # Missing indicator ì œì™¸
    non_missing_mask = [not name.endswith(MISSING_INDICATOR_SUFFIX) for name in feature_names]
    non_missing_indices = [i for i, mask in enumerate(non_missing_mask) if mask]
    feature_names_filtered = [feature_names[i] for i in non_missing_indices]
    X_filtered = X_sample[:, non_missing_indices]
    
    # Explainer ì„ íƒ (shap 0.32 í˜¸í™˜)
    def _make_kernel_explainer(bg_data):
        def predict_fn(x):
            if hasattr(model, 'predict_proba'):
                return model.predict_proba(x)[:, 1]
            return model.predict(x)
        return shap.KernelExplainer(predict_fn, bg_data)
    
    try:
        if model_type == 'tree':
            try:
                explainer = shap.TreeExplainer(model)
                shap_values = explainer.shap_values(X_sample)
                print(f"      TreeExplainer ì‚¬ìš©")
            except Exception as te:
                print(f"      TreeExplainer failed, using KernelExplainer: {te}")
                bg_size = min(50, len(X_sample))
                bg_indices = np.random.choice(len(X_sample), bg_size, replace=False)
                explainer = _make_kernel_explainer(X_sample[bg_indices])
                shap_values = explainer.shap_values(X_sample, nsamples=100)
        else:
            print(f"      KernelExplainer ì‚¬ìš© ({len(X_sample)} samples)...")
            bg_size = min(50, len(X_sample))
            bg_indices = np.random.choice(len(X_sample), bg_size, replace=False)
            explainer = _make_kernel_explainer(X_sample[bg_indices])
            shap_values = explainer.shap_values(X_sample, nsamples=200)
    except Exception as e:
        print(f"      âš ï¸ SHAP ê³„ì‚° ì˜¤ë¥˜: {e}")
        return None, None, None
    
    # SHAP values ì²˜ë¦¬ (2Dë¡œ ë³€í™˜)
    if isinstance(shap_values, list):
        shap_values = shap_values[1] if len(shap_values) == 2 else shap_values[0]
    
    if len(shap_values.shape) == 3:
        shap_values = shap_values[:, :, 1]
    
    # Missing indicator ì œì™¸
    shap_values_filtered = shap_values[:, non_missing_indices]
    
    return shap_values_filtered, X_filtered, feature_names_filtered


def plot_shap_comparison(
    models_shap_data: Dict[str, Tuple[np.ndarray, np.ndarray, List[str]]],
    save_path: str = None,
    top_n: int = 15,
    figsize: Tuple[int, int] = None
) -> plt.Figure:
    """
    ì—¬ëŸ¬ ëª¨ë¸ì˜ SHAP Beeswarm Plotì„ subplotìœ¼ë¡œ ë¹„êµ (SHAP ê¸°ë³¸ ì‹œê°í™” ì‚¬ìš©)
    
    Args:
        models_shap_data: {model_name: (shap_values, X, feature_names)}
        save_path: ì €ì¥ ê²½ë¡œ
        top_n: ìƒìœ„ Nê°œ featureë§Œ í‘œì‹œ
        figsize: Figure í¬ê¸° (Noneì´ë©´ ìë™ ê³„ì‚°)
    """
    if not HAS_SHAP:
        print("âš ï¸ SHAPê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    
    n_models = len(models_shap_data)
    if n_models == 0:
        print("âš ï¸ SHAP ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # (2, 3) ê³ ì • ë ˆì´ì•„ì›ƒ, ê° subplot 1:1 ë¹„ìœ¨ (3:2 ì „ì²´ ë¹„ìœ¨)
    n_rows = 2
    n_cols = 3
    if figsize is None:
        figsize = (24, 16)  # 3:2 ë¹„ìœ¨ â†’ ê° ì…€ (8, 8) â‰ˆ 1:1
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
    axes = axes.flatten()
    
    # ì•ŒíŒŒë²³ ë ˆì´ë¸”
    labels = ['A', 'B', 'C', 'D', 'E', 'F']
    
    for idx, (model_name, (shap_values, X, feature_names)) in enumerate(models_shap_data.items()):
        if idx >= len(axes):
            break
        
        ax = axes[idx]
        display_name = MODEL_DISPLAY_NAMES.get(model_name, model_name)
        
        # summary_plot í˜¸ì¶œ ì „ ê¸°ì¡´ axes ê¸°ë¡
        existing_axes = set(fig.get_axes())
        
        # í˜„ì¬ axesë¥¼ í™œì„±í™”
        plt.sca(ax)
        
        # SHAP ê¸°ë³¸ summary_plot ì‚¬ìš©
        shap.summary_plot(
            shap_values, X,
            feature_names=feature_names,
            max_display=top_n,
            show=False,
            plot_size=None  # subplot í¬ê¸° ì‚¬ìš©
        )
        
        # shapì´ ìë™ ìƒì„±í•œ colorbar axes ì œê±° (ë‚˜ì¤‘ì— ê³µìœ  colorbar ì¶”ê°€)
        new_axes = set(fig.get_axes()) - existing_axes - {ax}
        for cb_ax in new_axes:
            cb_ax.remove()
        
        # ì œëª© ì¶”ê°€
        ax.set_title(f'({labels[idx]}) {display_name}', fontsize=12, fontweight='bold')
        # 1:1 ë¹„ìœ¨ ì ìš©
        try:
            ax.set_box_aspect(1)
        except AttributeError:
            pass  # matplotlib < 3.6 í˜¸í™˜
        
    # ë¹ˆ subplot ìˆ¨ê¸°ê¸°
    for idx in range(len(models_shap_data), len(axes)):
        axes[idx].set_visible(False)
    
    # ê³µìœ  Feature Value colorbar ì¶”ê°€
    import matplotlib.cm as cm
    import matplotlib.colors as mcolors
    
    # SHAP ê¸°ë³¸ colormap ê°€ì ¸ì˜¤ê¸°
    try:
        from shap.plots.colors import red_blue as shap_cmap
    except ImportError:
        try:
            from shap import plots
            shap_cmap = plots.colors.red_blue
        except (ImportError, AttributeError):
            # SHAP 0.32 í˜¸í™˜: ê¸°ë³¸ blue-red colormap ìƒì„±
            shap_cmap = plt.cm.get_cmap('bwr')
    
    norm = mcolors.Normalize(vmin=0, vmax=1)
    sm = cm.ScalarMappable(cmap=shap_cmap, norm=norm)
    sm.set_array([])
    
    plt.tight_layout(rect=[0, 0, 0.92, 1], pad=1.5)
    
    # ì˜¤ë¥¸ìª½ ì—¬ë°±ì— colorbar ë°°ì¹˜
    cbar_ax = fig.add_axes([0.935, 0.15, 0.015, 0.7])  # [left, bottom, width, height]
    cbar = fig.colorbar(sm, cax=cbar_ax)
    cbar.set_ticks([0, 1])
    cbar.set_ticklabels(['Low', 'High'], fontsize=11)
    cbar.set_label('Feature Value', fontsize=13, labelpad=8)
    cbar.ax.tick_params(length=0)
    
    if save_path:
        _save_figure(fig, save_path, pad_inches=0.3)
    
    return fig


def create_comparison_figures(
    models_dir: str,
    data_dir: str,
    output_dir: str
) -> None:
    """
    ëª¨ë“  ë¹„êµ Figure ìƒì„±
    
    Args:
        models_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ë°ì´í„° ë¡œë“œ
    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))
    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))
    
    # Feature names ë¡œë“œ
    feature_names_path = os.path.join(data_dir, 'feature_names.txt')
    if os.path.exists(feature_names_path):
        with open(feature_names_path, 'r') as f:
            feature_names = [line.strip() for line in f.readlines()]
    else:
        feature_names = [f'feature_{i}' for i in range(X_test.shape[1])]
    
    print(f"\nğŸ“Š ëª¨ë¸ ë¹„êµ Figure ìƒì„±")
    print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ ìƒ˜í”Œ")
    print(f"   ì–‘ì„± í´ë˜ìŠ¤ ë¹„ìœ¨: {y_test.mean():.3f}")
    
    # ëª¨ë¸ ì°¾ê¸°
    models = find_models(models_dir)
    print(f"   ë°œê²¬ëœ ëª¨ë¸: {len(models)}ê°œ")
    
    if len(models) == 0:
        print("âš ï¸ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê° ëª¨ë¸ì— ëŒ€í•´ ë°ì´í„° ìˆ˜ì§‘
    roc_data = {}
    pr_data = {}
    cal_data = {}
    shap_data = {}
    loaded_models = {}
    
    for model_name, model_path in models.items():
        print(f"\n   ğŸ“ˆ {MODEL_DISPLAY_NAMES.get(model_name, model_name)} ì²˜ë¦¬ ì¤‘...")
        
        try:
            model = load_model(model_path)
            if model is None:
                print(f"      âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                continue
            loaded_models[model_name] = model
            y_prob = get_predictions(model, X_test)
            
            # ROC
            fpr, tpr, _ = roc_curve(y_test, y_prob)
            auc_score = roc_auc_score(y_test, y_prob)
            roc_data[model_name] = (fpr, tpr, auc_score)
            
            # PR
            precision, recall, _ = precision_recall_curve(y_test, y_prob)
            ap_score = average_precision_score(y_test, y_prob)
            pr_data[model_name] = (recall, precision, ap_score)
            
            # Calibration
            prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10, strategy='uniform')
            brier = brier_score_loss(y_test, y_prob)
            cal_data[model_name] = (prob_true, prob_pred, brier)
            
            print(f"      AUROC: {auc_score:.3f}, AUPRC: {ap_score:.3f}, Brier: {brier:.3f}")
            
        except Exception as e:
            print(f"      âš ï¸ ì˜¤ë¥˜: {e}")
            continue
    
    # Figure ìƒì„±
    baseline = y_test.mean()
    
    # ê°œë³„ Figure
    print(f"\nğŸ“Š ê°œë³„ Figure ìƒì„±...")
    plot_roc_comparison(roc_data, 
                        save_path=os.path.join(output_dir, 'comparison_roc.png'))
    plot_pr_comparison(pr_data, baseline=baseline,
                       save_path=os.path.join(output_dir, 'comparison_pr.png'))
    plot_calibration_comparison(cal_data,
                                save_path=os.path.join(output_dir, 'comparison_calibration.png'))
    
    # ê²°í•© Figure (ë…¼ë¬¸ìš©) - png, tiff, pdf 3ì¢… ìë™ ì €ì¥
    print(f"\nğŸ“Š ë…¼ë¬¸ìš© ê²°í•© Figure ìƒì„±...")
    plot_combined_comparison(roc_data, pr_data, cal_data, baseline=baseline,
                             save_path=os.path.join(output_dir, 'comparison_combined.png'))
    
    # SHAP Comparison Figure ìƒì„±
    if HAS_SHAP:
        print(f"\nğŸ“Š SHAP Comparison Figure ìƒì„± ì¤‘...")
        
        for model_name, model in loaded_models.items():
            print(f"   ğŸ“ˆ {MODEL_DISPLAY_NAMES.get(model_name, model_name)} SHAP ê³„ì‚° ì¤‘...")
            try:
                result = compute_shap_values(model, X_test, feature_names, y=y_test, max_samples=1000)
                if result[0] is not None:
                    shap_data[model_name] = result
            except Exception as e:
                print(f"      âš ï¸ SHAP ì˜¤ë¥˜: {e}")
        
        if len(shap_data) > 0:
            # SHAP Comparison Figure - png, tiff, pdf 3ì¢… ìë™ ì €ì¥
            plot_shap_comparison(shap_data,
                                save_path=os.path.join(output_dir, 'comparison_shap.png'))
    else:
        print("\nâš ï¸ SHAPê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ SHAP Comparison Figureë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    print(f"\nâœ… ëª¨ë“  ë¹„êµ Figure ìƒì„± ì™„ë£Œ!")
    print(f"   ì €ì¥ ìœ„ì¹˜: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description='ëª¨ë¸ ë¹„êµ ì‹œê°í™”')
    parser.add_argument('--models-dir', type=str, default='../models',
                        help='ëª¨ë¸ ë””ë ‰í† ë¦¬')
    parser.add_argument('--data-dir', type=str, default='../data/processed',
                        help='ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', type=str, default='../results/comparison',
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    create_comparison_figures(
        models_dir=args.models_dir,
        data_dir=args.data_dir,
        output_dir=args.output
    )


if __name__ == '__main__':
    main()
