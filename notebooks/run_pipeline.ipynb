{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Prediction Model Pipeline (Multi-Target)\n",
    "\n",
    "Machine learning pipeline for predicting diabetes incidence in gallstone patients.\n",
    "\n",
    "**Targets:**\n",
    "- `outA`: Primary outcome\n",
    "- `out2`: Secondary outcome\n",
    "\n",
    "**Execution Order:**\n",
    "1. Environment Setup\n",
    "2. Generate Dummy Data (Optional)\n",
    "3. For each target:\n",
    "   - Baseline Characteristics (Table 1)\n",
    "   - Data Preprocessing\n",
    "   - Model Training (GridSearchCV)\n",
    "   - Model Evaluation & SHAP Analysis\n",
    "   - Performance Comparison Table\n",
    "   - Model Comparison Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set project root\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "# Add code directory to path\n",
    "CODE_DIR = os.path.join(PROJECT_ROOT, 'code')\n",
    "if CODE_DIR not in sys.path:\n",
    "    sys.path.insert(0, CODE_DIR)\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Code Directory: {CODE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Data paths\n",
    "    'data_path': os.path.join(PROJECT_ROOT, 'data', 'dummy_diabetes_data.csv'),\n",
    "    \n",
    "    # Target variables (will run for each)\n",
    "    'targets': ['outA', 'out2'],\n",
    "    \n",
    "    # Preprocessing settings\n",
    "    'add_missing_indicator': True,\n",
    "    'missing_threshold': 0.05,\n",
    "    \n",
    "    # Training settings\n",
    "    'models': ['decision_tree', 'random_forest', 'xgboost', 'catboost', 'ann'],\n",
    "    'cv_folds': 5,\n",
    "    'scoring': 'roc_auc',\n",
    "    'small_grid': True,  # True: quick test, False: full grid\n",
    "    \n",
    "    # Bootstrap settings\n",
    "    'n_bootstrap': 1000,\n",
    "}\n",
    "\n",
    "def get_paths(target):\n",
    "    \"\"\"Get paths for a specific target\"\"\"\n",
    "    return {\n",
    "        'processed_dir': os.path.join(PROJECT_ROOT, 'data', 'processed', target),\n",
    "        'models_dir': os.path.join(PROJECT_ROOT, 'models', target),\n",
    "        'results_dir': os.path.join(PROJECT_ROOT, 'results', target),\n",
    "        'tables_dir': os.path.join(PROJECT_ROOT, 'results', target, 'tables'),\n",
    "        'comparison_dir': os.path.join(PROJECT_ROOT, 'results', target, 'comparison'),\n",
    "    }\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Targets: {CONFIG['targets']}\")\n",
    "print(f\"  Models: {CONFIG['models']}\")\n",
    "print(f\"  Small grid: {CONFIG['small_grid']}\")\n",
    "print(f\"  Bootstrap: {CONFIG['n_bootstrap']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Dummy Data (Optional)\n",
    "\n",
    "Skip this step if you have real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_DUMMY_DATA = True  # Set False to skip\n",
    "\n",
    "if GENERATE_DUMMY_DATA:\n",
    "    from make_dummy import generate_dummy_data\n",
    "    \n",
    "    df = generate_dummy_data(n_samples=10000)\n",
    "    df.to_csv(CONFIG['data_path'], index=False)\n",
    "    \n",
    "    print(f\"Dummy data saved: {CONFIG['data_path']}\")\n",
    "    print(f\"  Samples: {len(df):,}\")\n",
    "    print(f\"  Features: {len(df.columns)}\")\n",
    "    print(f\"\\n  Target distribution:\")\n",
    "    for target in CONFIG['targets']:\n",
    "        print(f\"  - {target}: {df[target].mean():.3f}\")\n",
    "else:\n",
    "    print(\"Skipping dummy data generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Run Pipeline for Each Target\n",
    "\n",
    "The following cells will run the complete pipeline for each target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all modules\n",
    "from create_table1 import create_all_tables\n",
    "from preprocessing import preprocess_and_save\n",
    "from train_gridsearch import ModelTrainer\n",
    "from evaluate import evaluate_model\n",
    "from create_performance_table import create_performance_table\n",
    "from create_comparison_figures import create_comparison_figures\n",
    "\n",
    "print(\"All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for all targets\n",
    "all_results = {}\n",
    "\n",
    "for target in CONFIG['targets']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸŽ¯ TARGET: {target}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get paths for this target\n",
    "    paths = get_paths(target)\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_path in paths.values():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    target_results = {\n",
    "        'paths': paths,\n",
    "        'training': {},\n",
    "        'evaluation': {},\n",
    "    }\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 2: Table 1\n",
    "    # =========================================================================\n",
    "    print(f\"\\n--- [{target}] Step 2: Baseline Characteristics ---\")\n",
    "    try:\n",
    "        tables = create_all_tables(\n",
    "            data_path=CONFIG['data_path'],\n",
    "            output_dir=paths['tables_dir'],\n",
    "            target_col=target\n",
    "        )\n",
    "        print(f\"  Table 1 saved to: {paths['tables_dir']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 3: Preprocessing\n",
    "    # =========================================================================\n",
    "    print(f\"\\n--- [{target}] Step 3: Preprocessing ---\")\n",
    "    try:\n",
    "        result = preprocess_and_save(\n",
    "            data_path=CONFIG['data_path'],\n",
    "            output_dir=paths['processed_dir'],\n",
    "            target_col=target,\n",
    "            add_missing_indicator=CONFIG['add_missing_indicator'],\n",
    "            missing_threshold=CONFIG['missing_threshold']\n",
    "        )\n",
    "        print(f\"  Preprocessing complete: {paths['processed_dir']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    X_train = np.load(os.path.join(paths['processed_dir'], 'X_train.npy'))\n",
    "    X_test = np.load(os.path.join(paths['processed_dir'], 'X_test.npy'))\n",
    "    y_train = np.load(os.path.join(paths['processed_dir'], 'y_train.npy'))\n",
    "    y_test = np.load(os.path.join(paths['processed_dir'], 'y_test.npy'))\n",
    "    \n",
    "    with open(os.path.join(paths['processed_dir'], 'feature_names.txt'), 'r') as f:\n",
    "        feature_names = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    print(f\"  Train: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
    "    print(f\"  Test:  {X_test.shape[0]:,} samples\")\n",
    "    print(f\"  Positive rate - Train: {y_train.mean():.3f}, Test: {y_test.mean():.3f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 4: Model Training\n",
    "    # =========================================================================\n",
    "    print(f\"\\n--- [{target}] Step 4: Model Training ---\")\n",
    "    \n",
    "    trainer = ModelTrainer(\n",
    "        cv=CONFIG['cv_folds'],\n",
    "        scoring=CONFIG['scoring'],\n",
    "        use_small_grid=CONFIG['small_grid']\n",
    "    )\n",
    "    \n",
    "    for model_name in CONFIG['models']:\n",
    "        print(f\"\\n  Training {model_name}...\")\n",
    "        try:\n",
    "            best_model, best_params = trainer.train_model(model_name, X_train, y_train)\n",
    "            eval_result = trainer.evaluate_model(model_name, X_test, y_test)\n",
    "            target_results['training'][model_name] = eval_result\n",
    "            print(f\"    AUROC: {eval_result['auroc']:.4f}, AUPRC: {eval_result['auprc']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "    \n",
    "    # Save models\n",
    "    saved_paths = trainer.save_all_models(paths['models_dir'], feature_names)\n",
    "    print(f\"\\n  Models saved: {list(saved_paths.keys())}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 5: Model Evaluation & SHAP\n",
    "    # =========================================================================\n",
    "    print(f\"\\n--- [{target}] Step 5: Model Evaluation & SHAP ---\")\n",
    "    \n",
    "    model_files = glob.glob(os.path.join(paths['models_dir'], '*_best_model.*'))\n",
    "    \n",
    "    for model_file in model_files:\n",
    "        model_name = os.path.basename(model_file).replace('_best_model', '').split('.')[0]\n",
    "        print(f\"\\n  Evaluating {model_name}...\")\n",
    "        try:\n",
    "            result = evaluate_model(\n",
    "                model_path=model_file,\n",
    "                data_dir=paths['processed_dir'],\n",
    "                output_dir=paths['results_dir'],\n",
    "                model_name=model_name\n",
    "            )\n",
    "            target_results['evaluation'][model_name] = result\n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 6: Performance Table\n",
    "    # =========================================================================\n",
    "    print(f\"\\n--- [{target}] Step 6: Performance Table ---\")\n",
    "    try:\n",
    "        model_paths = glob.glob(os.path.join(paths['models_dir'], '*_best_model.*'))\n",
    "        performance_table = create_performance_table(\n",
    "            model_paths=model_paths,\n",
    "            data_dir=paths['processed_dir'],\n",
    "            n_bootstrap=CONFIG['n_bootstrap'],\n",
    "            output_path=os.path.join(paths['tables_dir'], 'model_performance.xlsx')\n",
    "        )\n",
    "        target_results['performance_table'] = performance_table\n",
    "        print(f\"  Performance table saved: {paths['tables_dir']}/model_performance.xlsx\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 7: Comparison Figures\n",
    "    # =========================================================================\n",
    "    print(f\"\\n--- [{target}] Step 7: Comparison Figures ---\")\n",
    "    try:\n",
    "        create_comparison_figures(\n",
    "            models_dir=paths['models_dir'],\n",
    "            data_dir=paths['processed_dir'],\n",
    "            output_dir=paths['comparison_dir']\n",
    "        )\n",
    "        print(f\"  Comparison figures saved: {paths['comparison_dir']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    \n",
    "    # Store results\n",
    "    all_results[target] = target_results\n",
    "    print(f\"\\nâœ… Target {target} complete!\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… All targets complete!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of training results\n",
    "print(\"Training Results Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for target in CONFIG['targets']:\n",
    "    if target in all_results:\n",
    "        print(f\"\\nðŸŽ¯ Target: {target}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Model':<20} {'AUROC':>10} {'AUPRC':>10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        training_results = all_results[target].get('training', {})\n",
    "        for model_name, result in sorted(training_results.items(), \n",
    "                                          key=lambda x: x[1].get('auroc', 0), \n",
    "                                          reverse=True):\n",
    "            auroc = result.get('auroc', 0)\n",
    "            auprc = result.get('auprc', 0)\n",
    "            print(f\"{model_name:<20} {auroc:>10.4f} {auprc:>10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance tables\n",
    "for target in CONFIG['targets']:\n",
    "    if target in all_results and 'performance_table' in all_results[target]:\n",
    "        print(f\"\\nðŸŽ¯ Performance Table: {target}\")\n",
    "        print(\"=\" * 100)\n",
    "        display(all_results[target]['performance_table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison figures\n",
    "from IPython.display import Image, display\n",
    "\n",
    "for target in CONFIG['targets']:\n",
    "    if target in all_results:\n",
    "        paths = all_results[target]['paths']\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Comparison Figures: {target}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Combined figure\n",
    "        combined_fig = os.path.join(paths['comparison_dir'], 'comparison_combined.png')\n",
    "        if os.path.exists(combined_fig):\n",
    "            print(f\"\\nROC, PR, Calibration:\")\n",
    "            display(Image(filename=combined_fig, width=1200))\n",
    "        \n",
    "        # SHAP comparison\n",
    "        shap_fig = os.path.join(paths['comparison_dir'], 'comparison_shap.png')\n",
    "        if os.path.exists(shap_fig):\n",
    "            print(f\"\\nSHAP Comparison:\")\n",
    "            display(Image(filename=shap_fig, width=1200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output Files Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Pipeline Complete!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nOutput Files:\")\n",
    "print(f\"\\n  [Data]\")\n",
    "print(f\"    - Raw: {CONFIG['data_path']}\")\n",
    "\n",
    "for target in CONFIG['targets']:\n",
    "    if target in all_results:\n",
    "        paths = all_results[target]['paths']\n",
    "        print(f\"\\n  [{target}]\")\n",
    "        print(f\"    - Processed: {paths['processed_dir']}\")\n",
    "        print(f\"    - Models: {paths['models_dir']}\")\n",
    "        print(f\"    - Results: {paths['results_dir']}\")\n",
    "        print(f\"    - Tables: {paths['tables_dir']}\")\n",
    "        print(f\"    - Comparison: {paths['comparison_dir']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
